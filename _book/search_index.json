[["index.html", "Disciplined Convex Optimization in R Chapter 1 Introduction 1.1 Convex Problems in Statistics, Machine Learning 1.2 Non-convex Problems", " Disciplined Convex Optimization in R Anqi Fu, Balasubramanian Narasimhan, Stephen Boyd 2021-07-22 Chapter 1 Introduction Optimization problems are at the heart of statistical inference and machine learning, where a scalar criterion is either maximized (likelihood, for instance) or minimized (loss) to estimate a parameter of interest. Constraints might be added to narrow the search space or to ensure the solution has certain properties. The kinds of optimization problems we address in this tutorial have the following form: \\[ \\begin{array}{lll} \\text{minimize} &amp; f_0(x) &amp; \\\\ \\text{subject to} &amp; f_i(x) \\leq 0, &amp; i=1, \\ldots, M\\\\ &amp; Ax=b &amp; \\end{array} \\] with variable \\(x \\in {\\mathbf R}^n\\). Further, we ask that the objective and inequality constraints \\(f_0, \\ldots, f_M\\) are convex: for all \\(x\\), \\(y\\), \\(\\theta \\in [0,1]\\), \\[ \\begin{equation} f_i(\\theta x + (1-\\theta) y) \\leq \\theta f_i(x) + (1-\\theta) f_i(y) \\tag{1.1} \\end{equation} \\] i.e., graphs of \\(f_i\\) curve upward, the equality constraints are linear, i.e., they can be expressed as a simple matrix equation \\(Ax = b\\). Geometrically, a function is convex if the chord or line segment drawn from any point \\((x, f(x))\\) to another point \\((y, f(y))\\) lies above the graph of \\(f\\), as shown below. Figure 1.1: Source: https://www.solver.com A function is concave if \\(-f\\) is convex. Every linear function is both convex and concave. Convex optimization problems have many nice properties: The region containing solutions, if any, is convex as it is the intersection of convex constraint functions. Since the objective is also convex, if we find a local optimal solution, it is automatically the global optimal solution. There are a host of applications of this problem in many fields, including machine learning and statistics. 1.1 Convex Problems in Statistics, Machine Learning Maximum likelihood estimation with constraints OLS regression, nonnegative least squares, logistic regression Ridge, lasso, elastic-net regression Isotonic regression Huber (robust) regression Support vector machine Sparse inverse covariance estimation Maximum entropy and related problems New methods are being invented every year! 1.2 Non-convex Problems A non-convex optimization problem is any problem where the objective or any of the constraints are non-convex. Not every problem is convex, and in fact, the non-convex set is much larger. Non-convex problems are harder to solve in general. However, even when a problem is non-convex, one may be able to find a convex relaxation, an approximation to the original problem, that can yield useful results. Thus, developing tools to solve convex problems can go a long way. "],["getting-started.html", "Chapter 2 Getting Started 2.1 Solver Prerequisites", " Chapter 2 Getting Started The following are prerequisites. A recent R installation (version 3.5.x and up) will do. However, we highly recommend 4.0.5 so as to avoid any unforeseen problems with CVXR. CVXR version 1.0-9, which can be installed like any other package from CRAN. An IDE/editing environment such as RStudio, Emacs, or equivalent. If you use Emacs, we recommend the version packaged and distributed by Vincent Goulet as it includes many useful modes. (We find, however, that recent versions of these Emacs binaries do not include poly-R or the poly-markdown packages by default; youll have to install them like any other Emacs package.) All the additional libraries that are used in this tutorial. The following code snippet in your R session will install them for you. required_packages &lt;- c(&quot;tidyr&quot;, &quot;ggplot2&quot;, &quot;nnls&quot;, &quot;ISLR&quot;, &quot;glmnet&quot;, &quot;isotone&quot;, &quot;profvis&quot;, &quot;dplyr&quot;, &quot;survey&quot;, &quot;expm&quot;, &quot;RColorBrewer&quot;, &quot;kableExtra&quot;) install.packages(required_packages) 2.1 Solver Prerequisites CVXR comes with open source solvers, and most of the tutorial will use these built-in open source solvers. However, we will discuss use of commercial solvers, MOSEK and GUROBI in particular. These are optional and not necessary for the tutorial. If, however, you wish to follow along, you will need to install the (binary) solver packages provided by the vendors. Luckily, academic and evaluation single user licenses are available for free. Ensure you have the R development tools installed: Xcode on Macs, Rtools on Windows. 2.1.1 MOSEK Follow the general setup instructions. Obtain an evaluation or personal academic license. Follow the instructions provided to install the Rmosek package. Ensure that you can run any one of the R examples. 2.1.2 GUROBI Download GUROBI optimizer version 9.1.2. You will have to register for an account and accept the license agreement. Obtain an evaluation or academic licence. Install the gurobi R package using these instructions. Ensure that you can run any one of the R examples. "],["types-of-convex-optimization-problems.html", "Chapter 3 Types of Convex Optimization Problems 3.1 Linear Programs 3.2 Quadratic Programs 3.3 Cone Programs 3.4 Integer and Mixed Integer Programs", " Chapter 3 Types of Convex Optimization Problems Recall that a convex optimization problem is of the form \\[ \\begin{array}{lll} \\text{minimize} &amp; f_0(x) &amp; \\\\ \\text{subject to} &amp; f_i(x) \\leq 0, &amp; i=1, \\ldots, M \\\\ &amp; Ax=b &amp; \\tag{3.1} \\end{array} \\] for \\(x \\in {\\mathbf R}^n\\). 3.1 Linear Programs When the functions \\(f_i\\) for all \\(i\\) are linear in \\(x\\), the problem is called a linear program. These are much easier to solve, and the celebrated work of George Dantzig resulted in the Simplex Algorithm for linear programs. 3.2 Quadratic Programs When the objective \\(f_0\\) is quadratic in \\(x\\) and \\(f_1,\\ldots,f_M\\) are linear in \\(x\\), the problem is called a quadratic program. 3.3 Cone Programs When \\(f_i(x) \\leq 0\\) for all \\(i\\) constrain \\(x\\) to lie in a convex cone \\(C\\), the problem is called a cone program. Examples of \\(C\\) are the positive orthant \\(\\mathbf{R}^n\\), the set of positive semidefinite matrices \\(\\mathbf{S}_+^n\\), and the second-order cone \\(\\{(x,t) \\in \\mathbf{R}^n \\times \\mathbf{R}: \\|x\\| \\leq t\\}\\). 3.4 Integer and Mixed Integer Programs When all elements of \\(x\\) are constrained to be integers, or even binary (\\(\\{0,1\\}\\)) values, the problem is called an integer program. If only a few elements of \\(x\\) are constrained to be integers, then the problem is a mixed integer program. These problems are not convex, but can be relaxed (e.g., we replace \\(x \\in \\{0,1\\}\\) with \\(x \\in [0,1]\\)). "],["solving-convex-problems.html", "Chapter 4 Solving Convex Problems 4.1 Verifying Convexity 4.2 Domain Specific Languages for Convex Optimization 4.3 References", " Chapter 4 Solving Convex Problems Although convex problems can look very difficult (nonlinear, even nondifferentiable), they can be solved very efficiently like a linear program. So, how does one solve such problems in R? One possibility is to match the form of the problem to an existing solver routine. For example, the well-known optimx package provides the following omnibus function: optimx(par, fn, gr=NULL, hess=NULL, lower=-Inf, upper=Inf, method=c(&quot;Nelder-Mead&quot;,&quot;BFGS&quot;), itnmax=NULL, hessian=FALSE, control=list(), ...) Here one has to specify a vector of initial parameters (par), the objective function (fn), optional gradient (gr) and Hessian functions (hess) depending on the method used, and upper and lower bounds for the solution. Obviously, the objective and the constraints must be supported by the optimx routines. Another possibility is to use a package such as ROI, which provides interfaces to a number of solvers, including solvers for convex problems. It offers an object-oriented framework for defining optimization problems, but one still has to explicitly identify the type of every objective and constraint. If you can transform your problem into a cone program and use a standard cone program solver, then you can use ROI in a straightforward way. 4.1 Verifying Convexity Even before attempting a solution, one has to verify a problem is convex. One can start with the basic definition @ref{eq:convex-function} or use first or second order conditions \\(\\nabla^2{f}\\succeq 0.\\) These often turn out to be tedious and hard to derive. Another possibility is to construct \\(f_i\\) out of a library of basic examples or atoms that are convex. These could be combined using calculus rules and transformations that preserve convexity, yielding a problem that is automatically verified to be convex. 4.2 Domain Specific Languages for Convex Optimization Domain Specific Languages (DSLs) are specialized languages for a particular application, implemented in a general purpose programming language. They have become useful for expressing, manipulating, and solving problems in specific contexts: circuit design (VHDL), graph layout (DOT), data (XML), etc. Over the last few years, specialized languages have become available for general convex optimization using the constructive approach discussed above. CVX and YALMIP, both implemented in Matlab CVXPY implemented in Python Convex.jl implemented in Julia CVXR implemented in R Such DSLs may result in code that is slightly slower, but they are extremely flexible and enable fast prototyping of novel methods. The last one, CVXR, is our focus and is described in a paper (Fu, B. Narasimhan, and Boyd 2020) that appears in the Journal of Statistical Software. 4.3 References References "],["disciplined-convex-programming.html", "Chapter 5 Disciplined Convex Programming 5.1 Basic Convex Functions 5.2 Calculus Rules 5.3 Examples", " Chapter 5 Disciplined Convex Programming All new systems of notation are such that one can accomplish nothing by means of them which would not also be accomplished without them; but the advantage is that when such a system of notation corresponds to the innermost essence of frequently occuring needs, one can solve the problems belonging in that category, indeed can mechanically solve them in cases so complicated that without such an aid even the genius becomes powerless. Thus it is with the invention of calculating by letters in general; thus it was with the differential calculus C. F. Gauss to Schumacher, May 15, 1843 Thats awesome! I was disappointed to not see a direct reference to DCP, but still, its pretty clear! Stephen Boyd, on Gauss letter to Schumacher, April 8, 2019 5.1 Basic Convex Functions The following are some basic convex functions. \\(x^p\\) for \\(p \\geq 1\\) or \\(p \\leq 0\\); \\(-x^p\\) for \\(0 \\leq p \\leq 1\\) \\(\\exp(x)\\), \\(-\\log(x)\\), \\(x\\log(x)\\) \\(a^Tx + b\\) \\(x^Tx\\); \\(x^Tx/y\\) for \\(y&gt;0\\); \\((x^Tx)^{1/2}\\) \\(||x||\\) (any norm) \\(\\max(x_1, x_2, \\ldots, x_n)\\), \\(\\log(e^{x_1}+ \\ldots + e^{x_n})\\) \\(\\log(\\Phi(x))\\), where \\(\\Phi\\) is Gaussian CDF \\(\\log(\\text{det}X^{-1})\\) for \\(X \\succ 0\\) 5.2 Calculus Rules Nonnegative Scaling: if \\(f\\) is convex and \\(\\alpha \\geq 0\\), then \\(\\alpha f\\) is convex Sum: if \\(f\\) and \\(g\\) are convex, so is \\(f+g\\) Affine Composition: if \\(f\\) is convex, so is \\(f(Ax+b)\\) Pointwise Maximum: if \\(f_1,f_2, \\ldots, f_m\\) are convex, so is \\(f(x) = \\underset{i}{\\text{max}}f_i(x)\\) Partial Minimization: if \\(f(x, y)\\) is convex and \\(C\\) is a convex set, then \\(g(x) = \\underset{y\\in C}{\\text{inf}}f(x,y)\\) is convex Composition: if \\(h\\) is convex and increasing and \\(f\\) is convex, then \\(g(x) = h(f(x))\\) is convex There are many other rules, but the above will get you far. 5.3 Examples Piecewise-linear function: \\(f(x) = \\underset{i}{\\text{max}}(a_i^Tx + b_i)\\) \\(l_1\\)-regularized least-squares cost: \\(||Ax-b||_2^2 + \\lambda ||x||_1\\) with \\(\\lambda \\geq 0\\) Sum of \\(k\\) largest elements of \\(x\\): \\(f(x) = \\sum_{i=1}^mx_i - \\sum_{i=1}^{m-k}x_{(i)}\\) Log-barrier: \\(-\\sum_{i=1}^m\\log(f_i(x))\\) (on \\(\\{x | f_i(x) &lt; 0\\}\\), \\(f_i\\) convex) Distance to convex set \\(C\\): \\(f(x) = \\text{dist}(x,C) =\\underset{y\\in C}{\\text{inf}}||x-y||_2\\) Except for log-barrier, these functions are nondifferentiable. "],["a-simple-regression-example.html", "Chapter 6 A Simple Regression Example 6.1 Goals 6.2 Ordinary Least-Squares Regression 6.3 The CVXR Formulation 6.4 Adding Constraints 6.5 Summary", " Chapter 6 A Simple Regression Example 6.1 Goals Basic introduction to CVXR Exercise on formulating a different objective, demonstrating how CVXR works with native R functions Exercises on formulating linear dependence constraints on estimates using linear algebra Exercises on formulating monotonicity constraints using CVXR atoms 6.2 Ordinary Least-Squares Regression Consider a simple linear regression problem, where it is desired to estimate a set of parameters using a least-squares criterion. We generate some synthetic data in which we know the model completely, i.e. \\[ Y = X\\beta + \\epsilon, \\] where \\(Y\\) is a \\(100\\times 1\\) vector, \\(X\\) is a \\(100\\times 10\\) matrix, \\(\\beta = [-4, -3, \\ldots ,4, 5]\\) is a \\(10\\times 1\\) vector, and \\(\\epsilon \\sim N(0, 1)\\). set.seed(123) n &lt;- 50; p &lt;- 10; beta &lt;- -4:5 # beta is just -4 through 5. X &lt;- matrix(rnorm(n * p), nrow=n) colnames(X) &lt;- paste0(&quot;beta_&quot;, beta) Y &lt;- X %*% beta + rnorm(n) Given the data \\(X\\) and \\(Y\\), we can estimate the \\(\\beta\\) vector using the lm function in R, which fits a standard regression model. ls.model &lt;- lm(Y ~ 0 + X) # There is no intercept in our model above m &lt;- matrix(coef(ls.model), ncol = 1) \\(\\beta_{1}\\) -3.9025328 \\(\\beta_{2}\\) -2.9683386 \\(\\beta_{3}\\) -1.8234267 \\(\\beta_{4}\\) -1.1562133 \\(\\beta_{5}\\) 0.0753986 \\(\\beta_{6}\\) 0.9224317 \\(\\beta_{7}\\) 1.8799511 \\(\\beta_{8}\\) 3.1107020 \\(\\beta_{9}\\) 4.1086503 \\(\\beta_{10}\\) 5.0947592 These are the least-squares estimates and can be seen to be reasonably close to the original \\(\\beta\\) values -4 through 5. 6.3 The CVXR Formulation The CVXR formulation states the above as an optimization problem: \\[ \\begin{array}{ll} \\underset{\\beta}{\\mbox{minimize}} &amp; \\|y - X\\beta\\|_2^2, \\end{array} \\] which directly translates into a problem that CVXR can solve as shown in the steps below. Step 0. Load the CVXR library library(CVXR, warn.conflicts=FALSE) Step 1. Define the variable to be estimated beta &lt;- Variable(p) Step 2. Define the objective to be optimized objective &lt;- Minimize(sum((Y - X %*% beta)^2)) Notice how the objective is specified using functions such as sum, *%*, and ^ that are familiar to R users despite the fact that beta is no ordinary R expression, but a CVXR expression. Step 3. Create a problem to solve problem &lt;- Problem(objective) Step 4. Solve it! result &lt;- solve(problem) Step 5. Examine solution status and obtain objective value and estimate ## OLS Solution Status: optimal, OLS Objective value: 34.134609 We can indeed satisfy ourselves that the results we get match those from lm. m &lt;- cbind(result$getValue(beta), coef(ls.model)) CVXR est. lm est. \\(\\beta_{1}\\) -3.9025328 -3.9025328 \\(\\beta_{2}\\) -2.9683386 -2.9683386 \\(\\beta_{3}\\) -1.8234267 -1.8234267 \\(\\beta_{4}\\) -1.1562133 -1.1562133 \\(\\beta_{5}\\) 0.0753986 0.0753986 \\(\\beta_{6}\\) 0.9224317 0.9224317 \\(\\beta_{7}\\) 1.8799511 1.8799511 \\(\\beta_{8}\\) 3.1107020 3.1107020 \\(\\beta_{9}\\) 4.1086503 4.1086503 \\(\\beta_{10}\\) 5.0947592 5.0947592 6.3.1 Exercise Modify the objective to perform least absolute deviation regression and solve the problem. Compare the results to OLS. Which objective has a lower value? Hint: In LAD regression, we minimize the sum of the absolute value of the residuals. Also, since the default solver OSQP has some issues at the time of this writing, do pass on the parameter solver = \"ECOS\" in your call to CVXR::solve. 6.3.1.1 Solution objective2 &lt;- Minimize(sum(abs(Y - X %*% beta))) problem2 &lt;- Problem(objective2) result2 &lt;- solve(problem2, solver = &quot;ECOS&quot;) cat(sprintf(&quot;LAD Solution Status: %s, LAD Objective value: %f\\n&quot;, result2$status, result2$value)) ## LAD Solution Status: optimal, LAD Objective value: 31.270018 m2 &lt;- cbind(result2$getValue(beta), coef(ls.model)) CVXR LAD est. lm est. \\(\\beta_{1}\\) -4.0324773 -3.9025328 \\(\\beta_{2}\\) -2.8267845 -2.9683386 \\(\\beta_{3}\\) -1.9473476 -1.8234267 \\(\\beta_{4}\\) -1.1859615 -1.1562133 \\(\\beta_{5}\\) 0.1886891 0.0753986 \\(\\beta_{6}\\) 0.9447007 0.9224317 \\(\\beta_{7}\\) 1.9321485 1.8799511 \\(\\beta_{8}\\) 3.0070658 3.1107020 \\(\\beta_{9}\\) 4.2535749 4.1086503 \\(\\beta_{10}\\) 5.0186776 5.0947592 cat(&quot;LAD objective value: %f, OLS objective value: %f\\n&quot;, result2$value, result$value) ## LAD objective value: %f, OLS objective value: %f ## 31.27002 34.13461 N.B. Note the reuse of beta in objective2. The value of beta will change depending on the problem context, and the function result$getValue() or result2$getValue() will account for the context as shown below. m3 &lt;- cbind(result$getValue(beta), result2$getValue(beta)) Problem 1 est. Problem 2 est. \\(\\beta_{1}\\) -3.9025328 -4.0324773 \\(\\beta_{2}\\) -2.9683386 -2.8267845 \\(\\beta_{3}\\) -1.8234267 -1.9473476 \\(\\beta_{4}\\) -1.1562133 -1.1859615 \\(\\beta_{5}\\) 0.0753986 0.1886891 \\(\\beta_{6}\\) 0.9224317 0.9447007 \\(\\beta_{7}\\) 1.8799511 1.9321485 \\(\\beta_{8}\\) 3.1107020 3.0070658 \\(\\beta_{9}\\) 4.1086503 4.2535749 \\(\\beta_{10}\\) 5.0947592 5.0186776 6.4 Adding Constraints On the surface, it appears that we have replaced one call to lm with at least five or six lines of new R code. On top of that, the code actually runs slower, so it is not clear what we really achieved. However, suppose we knew that the \\(\\beta\\)s were nonnegative and wished to take this fact into account in our model. This is nonnegative least squares regression, and lm would no longer do the job. In CVXR, the modified problem merely requires the addition of a constraint to the problem definition. problem &lt;- Problem(objective, constraints = list(beta &gt;= 0)) result &lt;- solve(problem) betaEstimate &lt;- result$getValue(beta) \\(\\beta_{1}\\) 0.000000 \\(\\beta_{2}\\) 0.000000 \\(\\beta_{3}\\) 0.000000 \\(\\beta_{4}\\) 0.000000 \\(\\beta_{5}\\) 1.190672 \\(\\beta_{6}\\) 1.460162 \\(\\beta_{7}\\) 1.817840 \\(\\beta_{8}\\) 3.147380 \\(\\beta_{9}\\) 5.828936 \\(\\beta_{10}\\) 5.287308 We can verify once again that these values are comparable to those obtained from another R package, say nnls. nnls.fit &lt;- nnls::nnls(X, Y)$x m &lt;- cbind(betaEstimate, nnls.fit) CVXR NNLS est. nnls est. \\(\\beta_{1}\\) 0.000000 0.000000 \\(\\beta_{2}\\) 0.000000 0.000000 \\(\\beta_{3}\\) 0.000000 0.000000 \\(\\beta_{4}\\) 0.000000 0.000000 \\(\\beta_{5}\\) 1.190672 1.190672 \\(\\beta_{6}\\) 1.460162 1.460162 \\(\\beta_{7}\\) 1.817840 1.817840 \\(\\beta_{8}\\) 3.147380 3.147380 \\(\\beta_{9}\\) 5.828936 5.828936 \\(\\beta_{10}\\) 5.287308 5.287308 6.4.1 Exercise Suppose it is known that \\(\\sum_{i=1}^4\\beta_i \\leq 0\\). Modify the original OLS problem to add this constraint. 6.4.1.1 Solution The obvious solution is to add a constraint of the form constraint1 &lt;- beta[1] + beta[2] + beta[3] + beta[4] &lt;= 0 but it is generally easier working with matrices in CVXR, and so we construct a row vector with zeros everywhere except in positions 1 through 4. A &lt;- matrix(c(rep(1, 4), rep(0, 6)), nrow = 1) \\(\\beta_{1}\\) \\(\\beta_{2}\\) \\(\\beta_{3}\\) \\(\\beta_{4}\\) \\(\\beta_{5}\\) \\(\\beta_{6}\\) \\(\\beta_{7}\\) \\(\\beta_{8}\\) \\(\\beta_{9}\\) \\(\\beta_{10}\\) 1 1 1 1 0 0 0 0 0 0 The sum constraint on \\(\\beta\\) is therefore \\[ A\\beta \\leq 0 \\] which we express in R as constraint1 &lt;- A %*% beta &lt;= 0 We are ready to solve the problem. problem &lt;- Problem(objective, constraints = list(constraint1)) ex1 &lt;- solve(problem) And we can get the estimates of \\(\\beta\\). betaEstimate &lt;- ex1$getValue(beta) \\(\\beta_{1}\\) -3.9025328 \\(\\beta_{2}\\) -2.9683386 \\(\\beta_{3}\\) -1.8234267 \\(\\beta_{4}\\) -1.1562133 \\(\\beta_{5}\\) 0.0753986 \\(\\beta_{6}\\) 0.9224317 \\(\\beta_{7}\\) 1.8799511 \\(\\beta_{8}\\) 3.1107020 \\(\\beta_{9}\\) 4.1086503 \\(\\beta_{10}\\) 5.0947592 6.4.2 Exercise Add an additional constraint to the previous exercise that \\(\\beta_i \\leq 4\\) for \\(i=5,\\ldots,10\\). 6.4.2.1 Solution We create a diagonal matrix with ones along the diagonal entries \\(i=5,\\ldots,10\\). B &lt;- diag(c(rep(0, 4), rep(1, 6))) \\(\\beta_{1}\\) \\(\\beta_{2}\\) \\(\\beta_{3}\\) \\(\\beta_{4}\\) \\(\\beta_{5}\\) \\(\\beta_{6}\\) \\(\\beta_{7}\\) \\(\\beta_{8}\\) \\(\\beta_{9}\\) \\(\\beta_{10}\\) \\(\\beta_{1}\\) 0 0 0 0 0 0 0 0 0 0 \\(\\beta_{2}\\) 0 0 0 0 0 0 0 0 0 0 \\(\\beta_{3}\\) 0 0 0 0 0 0 0 0 0 0 \\(\\beta_{4}\\) 0 0 0 0 0 0 0 0 0 0 \\(\\beta_{5}\\) 0 0 0 0 1 0 0 0 0 0 \\(\\beta_{6}\\) 0 0 0 0 0 1 0 0 0 0 \\(\\beta_{7}\\) 0 0 0 0 0 0 1 0 0 0 \\(\\beta_{8}\\) 0 0 0 0 0 0 0 1 0 0 \\(\\beta_{9}\\) 0 0 0 0 0 0 0 0 1 0 \\(\\beta_{10}\\) 0 0 0 0 0 0 0 0 0 1 So this new constraint is nothing but constraint2 &lt;- B %*% beta &lt;= 4 problem2 &lt;- Problem(objective, constraints = list(constraint1, constraint2)) ex2 &lt;- solve(problem2) betaEstimate &lt;- ex2$getValue(beta) \\(\\beta_{1}\\) -4.0635639 \\(\\beta_{2}\\) -2.9932080 \\(\\beta_{3}\\) -1.9214633 \\(\\beta_{4}\\) -1.0858480 \\(\\beta_{5}\\) 0.0684279 \\(\\beta_{6}\\) 0.8810457 \\(\\beta_{7}\\) 1.8415355 \\(\\beta_{8}\\) 3.0255314 \\(\\beta_{9}\\) 4.0000000 \\(\\beta_{10}\\) 4.0000000 6.4.3 Exercise Solve the OLS regression problem under the constraint that the \\(\\beta_i\\) are nonnegative and monotonically nondecreasing. Hint: What function in R computes lagged differences? 6.4.3.1 Solution This requires some additional knowledge about R and CVXR functions. The base::diff generic function generates lagged differences of any order. CVXR provides a method for the generic. So the monotonicity constraint can be succintly expressed as diff(beta) &gt;= 0. problem3 &lt;- Problem(objective, constraints = list(beta &gt;= 0, diff(beta) &gt;= 0)) ex3 &lt;- solve(problem3) betaEstimate &lt;- ex3$getValue(beta) \\(\\beta_{1}\\) -0.0003094 \\(\\beta_{2}\\) -0.0001897 \\(\\beta_{3}\\) 0.0002098 \\(\\beta_{4}\\) 0.0001500 \\(\\beta_{5}\\) 1.1687435 \\(\\beta_{6}\\) 1.4481293 \\(\\beta_{7}\\) 1.8421746 \\(\\beta_{8}\\) 3.2337443 \\(\\beta_{9}\\) 5.5646969 \\(\\beta_{10}\\) 5.5646783 6.4.4 Exercise Fit OLS with just the following order constraints on \\(\\beta\\): \\(\\beta_{i} \\leq \\beta_{i+1}\\) for \\(i=1,\\ldots, 4\\) and \\(\\beta_i \\geq \\beta_{i+1}\\) for \\(i=5,\\ldots,p\\). 6.4.4.1 Solution We have to combine all that we have learned earlier. D1 &lt;- cbind(diag(5), diag(0, 5)) D2 &lt;- cbind(matrix(0, 6, 4), diag(6)) constraints = list(diff(D1 %*% beta) &gt;= 0, diff(D2 %*% beta) &lt;= 0) problem4 &lt;- Problem(objective, constraints) ex4 &lt;- solve(problem4) betaEstimate &lt;- ex4$getValue(beta) \\(\\beta_{1}\\) -4.124667 \\(\\beta_{2}\\) -2.731246 \\(\\beta_{3}\\) -1.739471 \\(\\beta_{4}\\) -1.456388 \\(\\beta_{5}\\) 2.823590 \\(\\beta_{6}\\) 2.823590 \\(\\beta_{7}\\) 2.823590 \\(\\beta_{8}\\) 2.823590 \\(\\beta_{9}\\) 2.823590 \\(\\beta_{10}\\) 2.823590 6.5 Summary This introduction demonstrates a chief advantage of CVXR: flexibility. Users can quickly modify and re-solve a problem, which is ideal for prototyping and experimenting with new statistical methods. The CVXR syntax is simple and mathematically intuitive. Furthermore, CVXR combines seamlessly with native R code as well as several popular packages, allowing it to be incorporated easily into a larger analytical framework. The user is free to construct statistical estimators that are solutions to a convex optimization problem where there may not be a closed form solution or even an implementation. Later, we will see how such solutions can be used with resampling techniques like the bootstrap to estimate variability. "],["how-cvxr-works.html", "Chapter 7 How CVXR Works 7.1 Variables 7.2 Objectives, Constraints, and Problems 7.3 Solving the Problem 7.4 Solver Options", " Chapter 7 How CVXR Works Let us consider the nonnegative least squares regression example once again. beta &lt;- Variable(p) objective &lt;- Minimize(sum((Y - X %*% beta)^2)) constraints &lt;- list(beta &gt;= 0) problem &lt;- Problem(objective, constraints) result &lt;- solve(problem) solution_status &lt;- result$status objective_value &lt;- result$value beta_hat &lt;- result$getValue(beta) 7.1 Variables The CVXR::Variable function constructs an S4 class describing the argument of an optimization problem. Variable() specifies a 1-vector, essentially a scalar Variable(m) specifies an \\(m\\)-vector Variable(m, n) specifies an \\(m\\times n\\) matrix There are also S4 classes representing certain special constructs such as semidefinite matrices as we shall see later. 7.2 Objectives, Constraints, and Problems The objective function should yield a scalar value. CVXR provides the Minimize and Maximize functions that take an Expression (another S4 class) as an argument. Standard arithmetic and generics are overloaded so that one may use any R function in the construction. Constraints are specified as a list. One may construct this list directly or via some iterative computation, once again using various R and CVXR functions in the process. In the above problem, the objective is objective &lt;- Minimize(sum((Y - X %*% beta)^2)) which uses the CVXR::Minimize function along with standard R functions such as sum and squaring. This allows one to seamlessly work with all standard R constructs. However, the same objective may also be specified as objective &lt;- Minimize(sum_squares(Y - X %*% beta)) using the CVXR::sum_squares function. As you use CVXR more and more, you will need to refer to the CVXR functions list to learn about these built-in functions. A problem takes an objective and an optional constraint. It serves as the complete representation of the problem along with associated data, like the Y and X in the code snippet above. 7.3 Solving the Problem Calling the solve function on a problem sets several things in motion. The problem is verified for convexity. If it is not convex, the solve attempt fails with an error message and a non-optimal status. The problem along with the data is converted into a canonical form. The problem is analyzed and classified according to its type: LP, QP, SDP, etc. Among the available solvers, a suitable one is chosen that can handle the problem. Three open source solvers are built-in: Embedded Conic Solver (ECOS), Splitting Conic Solver (SCS), and Operator Splitting Quadratic Program Solver (OSQP), but there is also support for commercial solvers. The canonicalized data structures (matrices, cone dimensions) along with solver options, if any, are dispatched to the solver in appropriate form. Finally, the results from the solver along with some accessor functions for retrieving the solution and other quantities in the context of the solved problem are prepared and returned to the caller. There are several modes of failure that can occur. The problem may not be convex, and that is indicated via an error message. However, even when the problem is convex, the solver may not converge to a solution. The latter could be due to a number of reasons: tight tolerances, too few iterations, numerical issues, etc. Therefore, the solution status should always be examined. One option that can be very useful is verbosity, and this is specified by simply passing another parameter to CVXR::solve. result &lt;- solve(problem, verbose = TRUE) ## ----------------------------------------------------------------- ## OSQP v0.6.0 - Operator Splitting QP Solver ## (c) Bartolomeo Stellato, Goran Banjac ## University of Oxford - Stanford University 2019 ## ----------------------------------------------------------------- ## problem: variables n = 60, constraints m = 51 ## nnz(P) + nnz(A) = 604 ## settings: linear system solver = qdldl, ## eps_abs = 1.0e-05, eps_rel = 1.0e-05, ## eps_prim_inf = 1.0e-04, eps_dual_inf = 1.0e-04, ## rho = 1.00e-01 (adaptive), ## sigma = 1.00e-06, alpha = 1.60, max_iter = 10000 ## check_termination: on (interval 25), ## scaling: on, scaled_termination: off ## warm start: on, polish: on, time_limit: off ## ## iter objective pri res dua res rho time ## 1 0.0000e+00 3.23e+01 2.20e+04 1.00e-01 1.08e-04s ## 50 3.4135e+01 2.51e-07 2.97e-07 1.20e-02 2.03e-04s ## plsh 3.4135e+01 5.80e-15 1.12e-14 --------- 2.61e-04s ## ## status: solved ## solution polish: successful ## number of iterations: 50 ## optimal objective: 34.1346 ## run time: 2.61e-04s ## optimal rho estimate: 4.33e-03 7.4 Solver Options Solver options are unique to the chosen solver, so any arguments to CVXR::solve besides those documented here are simply passed along to the solver. The reference for the specific solver must be consulted to set these options. "],["logistic-regression.html", "Chapter 8 Logistic Regression 8.1 Goals 8.2 Logistic Regression Problem 8.3 Example 8.4 Exercise 8.5 References", " Chapter 8 Logistic Regression 8.1 Goals Formulating the logistic regression likelihood using CVXR atoms Example comparing CVXR results with R results from glm Exercise on extracting fitted values from CVXR logistic fit using lexically scoped CVXR facilities. 8.2 Logistic Regression Problem In logistic regression (James et al. 2013), the response \\(y_i\\) is binary: 0 or 1. (In a classification setting, the values of the response would represent class membership in one of two classes.) The conditional response given covariates \\(x\\) is modeled as \\[ y|x \\sim \\mbox{Bernoulli}(g_{\\beta}(x)), \\] where \\(g_{\\beta}(x) = \\frac{1}{1 + e^{-x^T\\beta}}\\) is the logistic function. We want to maximize the log-likelihood function, yielding the optimization problem \\[ \\begin{array}{ll} \\underset{\\beta}{\\mbox{maximize}} &amp; \\sum_{i=1}^m \\{ y_i\\log(g_{\\beta}(x_i)) + (1-y_i)\\log(1 - g_{\\beta}(x_i)) \\}. \\end{array} \\] One may be tempted to use log(1 + exp(X %*% beta)) as in conventional R syntax. However, this representation of \\(f(z)\\) violates the DCP composition rule, so the CVXR parser will reject the problem even though the objective is convex. Users who wish to employ a function that is convex, but not DCP compliant should check the documentation for a custom atom or consider a different formulation. CVXR provides the logistic atom as a shortcut for \\(f(z) = \\log(1 + e^z)\\). 8.3 Example We use example 4.6.2 of (James et al. 2013) with the Smarket data where a glm is fit as follows. library(ISLR) data(Smarket) glmfit &lt;- stats::glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial) The CVXR formulation merely has to specify the objective after setting up the data matrices appropriately. y &lt;- as.integer(Smarket$Direction) - 1L X &lt;- cbind(1, as.matrix(Smarket[, c(&quot;Lag1&quot;, &quot;Lag2&quot;, &quot;Lag3&quot;, &quot;Lag4&quot;, &quot;Lag5&quot;, &quot;Volume&quot;)])) p &lt;- ncol(X) beta &lt;- Variable(p) objective &lt;- -sum(X[y &lt;= 0, ] %*% beta) - sum(logistic(-X %*% beta)) problem &lt;- Problem(Maximize(objective)) result &lt;- solve(problem) beta_hat &lt;- result$getValue(beta) We can compare with the standard stats::glm estimate. CVXR GLM \\(\\beta_{0}\\) -0.1260091 -0.1260003 \\(\\beta_{1}\\) -0.0730788 -0.0730737 \\(\\beta_{2}\\) -0.0423043 -0.0423013 \\(\\beta_{3}\\) 0.0110857 0.0110851 \\(\\beta_{4}\\) 0.0093595 0.0093589 \\(\\beta_{5}\\) 0.0103137 0.0103131 \\(\\beta_{6}\\) 0.1354496 0.1354407 8.4 Exercise Standard stats::glm returns an object that has fitted values: glmfit$fitted.values. How would you compute the fitted values from CVXR? Hint: The result$getValue() evalutes expressions in the problem context. 8.4.1 Solution A key feature of CVXR is that it exploits lexical scoping built into R. So one can evaluate various functions of the variables that are solutions to the optimization problem. The fitted values can be computed using fitted_values &lt;- 1 / (1 + exp(result$getValue(-X %*% beta))) We can also satisfy ourselves that the fitted values match the glm estimate, by computing the sum of squared differences. sum((fitted_values - glmfit$fitted.values))^2 ## [1] 1.779124e-06 Similarly, the log-odds, \\(X\\hat{\\beta}\\) , where \\(\\hat{\\beta}\\) is the logistic regression estimate, can be computed as follows. log_odds &lt;- result$getValue(X %*% beta) 8.5 References References "],["isotonic-regression.html", "Chapter 9 Isotonic Regression 9.1 Goals 9.2 Pituitary Data Example 9.3 Handling Ties", " Chapter 9 Isotonic Regression 9.1 Goals Formulate the isotonic regression objective using some new CVXR atoms Compare with results from isotone package Exercise on handling ties, the secondary method of isotone, using CVXR atoms Exercise on handling ties, the tertiary method of isotone, using CVXR atoms Isotonic regression is regression with monotonicity constraints. There are several packages in R to fit isotonic regression models. In this example, we consider isotone, which uses a pooled-adjacent-violators algorithm (PAVA) and active set methods to perform the fit. 9.2 Pituitary Data Example We will use data from the isotone package (de Leeuw, Hornik, and Mair 2009) on the size of pituitary fissures for 11 subjects between 8 and 14 years of age. data(&quot;pituitary&quot;) str(pituitary) ## &#39;data.frame&#39;: 11 obs. of 2 variables: ## $ age : num 8 8 8 10 10 10 12 12 12 14 ... ## $ size: num 21 23.5 23 24 21 25 21.5 22 19 23.5 ... Since the size is expected to increase with age, an isotonic fit is suggested, so we fit using the isotone package. res_p &lt;- with(pituitary, gpava(age, size)) The CVXR formulation expresses this pretty much in the mathematical form. We define a variable x of size n, the number of observations. The objective to be minimized is the least-squares error (cvxr_norm), yet another way of specifying least-squares loss. The monotonicity is specified using the diff function. 9.2.1 Exercise Can you explain why CVXR provide functions such as cvxr_norm and p_norm rather than just plain pnorm? 9.2.1.1 Solution In R, pnorm is already defined and refers to the density of the normal distribution. So we use a new generic cvxr_norm or p_norm (see CVXR functions to avoid confusion. The function cvxr_norm provides some specialized norms for matrices, whereas p_norm allows one to specify \\(p\\). x_p &lt;- with(pituitary, { n &lt;- length(size) x &lt;- Variable(n) objective &lt;- Minimize(cvxr_norm(size - x, 2)) constraint &lt;- list(diff(x) &gt;= 0) problem &lt;- Problem(objective, constraint) result &lt;- solve(problem) result$getValue(x) }) As the output below shows, the results are very close. print_matrix(cbind(res_p$x, x_p), col_names = c(&quot;isotone&quot;, &quot;CVXR&quot;)) isotone CVXR 21.000 20.99998 22.375 22.37500 22.375 22.37500 22.375 22.37500 22.375 22.37500 22.375 22.37500 22.375 22.37500 22.375 22.37500 22.375 22.37500 23.500 23.50001 25.000 24.99999 9.3 Handling Ties Package isotone provides additional methods for handling tied data besides the default ties = \"primary\" method; ties = \"secondary\" enforces equality within ties, and ties = \"tertiary\" enforces monotonicity on the means. (The latter may cause individual fits to be non-monotonic.) res_s &lt;- with(pituitary, gpava(age, size, ties = &quot;secondary&quot;)) res_t &lt;- with(pituitary, gpava(age, size, ties = &quot;tertiary&quot;)) 9.3.1 Exercise Implement the secondary method of ties using CVXR and compare the results with the isotone package. 9.3.1.1 Solution The secondary method for ties just requires an additional constraint to enforce equality within tied values; no other modification is necessary. We do this below by figuring out the tied observation indices using base::split and forcing those x values to be equal (i.e. diff == 0). x_s &lt;- with(pituitary, { n &lt;- length(size) x &lt;- Variable(n) objective &lt;- Minimize(p_norm(size - x, 2)) secondary_constraints &lt;- lapply(base::split(x = seq_len(n), f = age), function(i) diff(x[i]) == 0) constraint &lt;- c(diff(x) &gt;= 0, secondary_constraints) problem &lt;- Problem(objective, constraint) solve(problem)$getValue(x) }) Heres the comparison table. Isotone (S) CVXR (S) 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 22.22222 24.25000 24.25001 24.25000 24.25001 9.3.2 Exercise Implement the tertiary method for ties using CVXR and compare with the isotone package. 9.3.2.1 Solution The tertiary method requires computing the block means for use in enforcing monotonicity. We call the CVXR::vstack function to create a single vector of the block means. Basically, CVXR::hstack is the equivalent of base::cbind and CVXR::vstack is the equivalent of base::rbind. x_t &lt;- with(pituitary, { n &lt;- length(size) x &lt;- Variable(n) objective &lt;- Minimize(p_norm(size - x, 2)) blocks &lt;- base::split(x = seq_len(n), f = pituitary$age) block_means &lt;- lapply(blocks, function(i) { v &lt;- numeric(n) v[i] &lt;- 1.0 / length(i) matrix(v, nrow = 1) %*% x }) block_mean_vector &lt;- do.call(vstack, block_means) constraint &lt;- list(diff(block_mean_vector) &gt;= 0) problem &lt;- Problem(objective, constraint) solve(problem)$getValue(x) }) Heres the comparison table. Isotone (T) CVXR (T) 20.72222 20.72224 23.22222 23.22224 22.72222 22.72224 22.88889 22.88890 19.88889 19.88890 23.88889 23.88890 22.88889 22.88890 23.38889 23.38890 20.38889 20.38890 23.50000 23.49994 25.00000 24.99994 References "],["lasso-and-elastic-net.html", "Chapter 10 Lasso and Elastic Net 10.1 Goals 10.2 Regularized Regression", " Chapter 10 Lasso and Elastic Net 10.1 Goals Formulate lasso and elastic net regression models Compare with results from glmnet package Use loss functions besides squared loss with elastic net penalty 10.2 Regularized Regression Often in applications, we encounter problems that require regularization to prevent overfitting, introduce sparsity, facilitate variable selection, or impose prior distributions on parameters. Two of the most common regularization functions are the \\(l_1\\)-norm and squared \\(l_2\\)-norm, combined in the elastic net regression model (H. Zou (2005) and Friedman, Hastie, and Tibshirani (2010)). \\[ \\begin{array}{ll} \\underset{\\beta}{\\mbox{minimize}} &amp; \\frac{1}{2m}\\|y - X\\beta\\|_2^2 + \\lambda(\\frac{1-\\alpha}{2}\\|\\beta\\|_2^2 + \\alpha\\|\\beta\\|_1). \\end{array} \\] Here \\(\\lambda \\geq 0\\) is the overall regularization weight and \\(\\alpha \\in [0,1]\\) controls the relative \\(l_1\\) versus squared \\(l_2\\) penalty. Thus, this model encompasses both ridge (\\(\\alpha = 0\\)) and lasso (\\(\\alpha = 1\\)) regression. It is convenient to define a function that calculates just the regularization term given the variable and penalty parameters. This modular approach will allow us to easily incorporate elastic net regularization into other regression models as we will see below. #&#39; Define the elastic penalty #&#39; @param beta the arg min variable #&#39; @param lambda the penalization parameter #&#39; @param alpha the elastic net parameter, 0 = ridge, 1 = lasso elastic_penalty &lt;- function(beta, lambda = 0, alpha = 0) { ridge &lt;- (1 - alpha) / 2 * sum_squares(beta) lasso &lt;- alpha * cvxr_norm(beta, 1) lambda * (lasso + ridge) } We generate some synthetic sparse data for this example. ## Problem data set.seed(4321) p &lt;- 10 n &lt;- 500 DENSITY &lt;- 0.25 # Fraction of non-zero beta beta_true &lt;- matrix(rnorm(p), ncol = 1) idxs &lt;- sample.int(p, size = floor((1 - DENSITY) * p), replace = FALSE) beta_true[idxs] &lt;- 0 sigma &lt;- 45 X &lt;- matrix(rnorm(n * p, sd = 5), nrow = n, ncol = p) eps &lt;- matrix(rnorm(n, sd = sigma), ncol = 1) Y &lt;- X %*% beta_true + eps We fit the elastic net model for several values of \\(\\lambda\\). TRIALS &lt;- 10 beta_vals &lt;- matrix(0, nrow = p, ncol = TRIALS) lambda_vals &lt;- 10^seq(-2, log10(50), length.out = TRIALS) beta &lt;- Variable(p) loss &lt;- sum_squares(Y - X %*% beta) / (2 * n) ## Elastic-net regression LASSO alpha &lt;- 1 beta_vals &lt;- sapply(lambda_vals, function (lambda) { obj &lt;- loss + elastic_penalty(beta, lambda, alpha) prob &lt;- Problem(Minimize(obj)) result &lt;- solve(prob) result$getValue(beta) }) We can now get a table of the coefficients. \\(\\lambda = 0.010\\) \\(\\lambda = 0.026\\) \\(\\lambda = 0.066\\) \\(\\lambda = 0.171\\) \\(\\lambda = 0.441\\) \\(\\lambda = 1.135\\) \\(\\lambda = 2.924\\) \\(\\lambda = 7.533\\) \\(\\lambda = 19.408\\) \\(\\lambda = 50.000\\) \\(\\beta_{1}\\) 0.891 0.890 0.888 0.883 0.870 0.836 0.747 0.553 0.038 0 \\(\\beta_{2}\\) 0.040 0.040 0.039 0.036 0.028 0.009 0.000 0.000 0.000 0 \\(\\beta_{3}\\) 0.301 0.300 0.299 0.294 0.283 0.254 0.181 0.000 0.000 0 \\(\\beta_{4}\\) -0.579 -0.578 -0.576 -0.572 -0.561 -0.533 -0.458 -0.265 0.000 0 \\(\\beta_{5}\\) 0.386 0.385 0.384 0.380 0.371 0.347 0.283 0.114 0.000 0 \\(\\beta_{6}\\) 1.538 1.538 1.536 1.531 1.519 1.487 1.403 1.186 0.627 0 \\(\\beta_{7}\\) 0.181 0.180 0.178 0.174 0.163 0.136 0.065 0.000 0.000 0 \\(\\beta_{8}\\) -0.134 -0.134 -0.132 -0.127 -0.114 -0.082 0.000 0.000 0.000 0 \\(\\beta_{9}\\) 0.939 0.938 0.936 0.931 0.918 0.885 0.794 0.570 0.026 0 \\(\\beta_{10}\\) -0.329 -0.328 -0.327 -0.322 -0.312 -0.284 -0.215 -0.030 0.000 0 We plot the coefficients against the regularization. plot(0, 0, type = &quot;n&quot;, main = &quot;CVXR Regularization Path for Lasso Regression&quot;, xlab = &quot;Log Lambda&quot;, ylab = &quot;Coefficients&quot;, ylim = c(-1, 2), xlim = c(-4, 4)) matlines(log(lambda_vals), t(beta_vals)) We then compare with the glmnet results. model_net &lt;- glmnet(X, Y, family = &quot;gaussian&quot;, alpha = alpha, lambda = lambda_vals, standardize = FALSE, intercept = FALSE, thresh = 1e-8) ## Reverse order to match beta_vals coef_net &lt;- as.data.frame(as.matrix(coef(model_net)[-1, seq(TRIALS, 1, by = -1)])) \\(\\lambda = 0.010\\) \\(\\lambda = 0.026\\) \\(\\lambda = 0.066\\) \\(\\lambda = 0.171\\) \\(\\lambda = 0.441\\) \\(\\lambda = 1.135\\) \\(\\lambda = 2.924\\) \\(\\lambda = 7.533\\) \\(\\lambda = 19.408\\) \\(\\lambda = 50.000\\) \\(\\beta_{1}\\) 0.891 0.890 0.888 0.883 0.870 0.836 0.747 0.553 0.038 0 \\(\\beta_{2}\\) 0.040 0.040 0.039 0.036 0.028 0.009 0.000 0.000 0.000 0 \\(\\beta_{3}\\) 0.301 0.300 0.299 0.294 0.283 0.254 0.181 0.000 0.000 0 \\(\\beta_{4}\\) -0.579 -0.578 -0.576 -0.572 -0.561 -0.533 -0.458 -0.265 0.000 0 \\(\\beta_{5}\\) 0.386 0.385 0.384 0.380 0.371 0.347 0.283 0.114 0.000 0 \\(\\beta_{6}\\) 1.538 1.538 1.536 1.531 1.519 1.487 1.403 1.186 0.627 0 \\(\\beta_{7}\\) 0.181 0.180 0.178 0.174 0.163 0.136 0.065 0.000 0.000 0 \\(\\beta_{8}\\) -0.134 -0.134 -0.132 -0.127 -0.114 -0.082 0.000 0.000 0.000 0 \\(\\beta_{9}\\) 0.939 0.938 0.936 0.931 0.918 0.885 0.794 0.570 0.026 0 \\(\\beta_{10}\\) -0.329 -0.328 -0.327 -0.322 -0.312 -0.284 -0.215 -0.030 0.000 0 10.2.1 Exercise A Stack Overflow Question! Im attempting to match some simple results in R using glmnet and CVXR. I have the following code. library(glmnet) data(QuickStartExample) x &lt;- QuickStartExample$x y &lt;- QuickStartExample$y catn &lt;- function(...) cat(..., &quot;\\n&quot;) objective_value &lt;- function(y, x, coefs, lambda, alpha) { n &lt;- nrow(x) ridge &lt;- sum(coefs^2) ; l1 &lt;- sum(abs(coefs)) sum((y - (x %*% coefs))^2) / (2 * n) + lambda * ((1 - alpha) / 2 * ridge + alpha * l1) } alpha &lt;- 0; lambda &lt;- 1; fit &lt;- glmnet(x, y, intercept=F, standardize=F, lambda=1, alpha=0) which gives me one set of coefficients and the objective function value 2.559086 via objective_value(y, x, coef(fit)[-1, ], lambda, alpha) ## [1] 2.573793 but this CVXR code beta &lt;- Variable(20) elastic_reg &lt;- function(beta, lambda = 0, alpha = 0) { ridge &lt;- (1 - alpha) * sum(beta^2) * .5 lasso &lt;- alpha * p_norm(beta, 1) lambda * (lasso + ridge) } loss &lt;- sum((y - x %*% beta)^2)/(2*length(y)) obj &lt;- loss + elastic_reg(beta, lambda = 1, 0) prob &lt;- Problem(Minimize(obj)) result &lt;- solve(prob) print(result$value) ## [1] 2.172438 gives a different objective value 2.859259 and a somewhat different set of coefficients. Can you help? 10.2.1.1 Solution To compare glmnet results to CVXR for the Gaussian case, it is advisable to standardize the response per the glmnet documentation. Note also that glmnet uses \\(n\\) rather than \\(n-1\\) in the denominator for \\(y\\). This will ensure that the \\(\\lambda\\) is on the same scale as shown below. catn &lt;- function(...) cat(..., &quot;\\n&quot;) ## Standardize the y y_s &lt;- local({ n &lt;- length(y) m &lt;- mean(y); s &lt;- as.numeric(sqrt(var(y) * (n - 1) / n)); result &lt;- (y - m) / s ## scale using 1/n attr(result, &quot;scaled:center&quot;) &lt;- m attr(result, &quot;scaled:scale&quot;) &lt;- s result }) We can do a comparison on the standardized \\(y\\). First, the glmnet answer: ## STANDARDIZED COMPARISON fit_s &lt;- glmnet(x, y_s, intercept=F, standardize=F, lambda = lambda, alpha=alpha) catn(&quot;Glmnet objective (scaled y)&quot;, objective_value(y_s, x, coef(fit_s)[-1], lambda, alpha)) ## Glmnet objective (scaled y) 0.2504412 Next, the CVXR answer: elastic_reg &lt;- function(beta, lambda = 0, alpha = 0) { ridge &lt;- (1 - alpha) / 2 * sum_squares(beta) lasso &lt;- alpha * p_norm(beta, 1) lambda * (lasso + ridge) } loss &lt;- sum_squares(y_s - x %*% beta) / (2 * nrow(x)) obj &lt;- loss + elastic_reg(beta, lambda = lambda, alpha) prob &lt;- Problem(Minimize(obj)) beta_est &lt;- solve(prob)$getValue(beta) catn(&quot;CVXR objective (scaled y):&quot;, objective_value(y_s, x, beta_est, lambda, alpha)) ## CVXR objective (scaled y): 0.2504412 To work on the non-standardized scale, we need to match the lamba values as noted by the glmnet authors in Appendix 2 of the package vignette (Friedman, Hastie, and Tibshirani 2010). ## NONSTANDARDIZED COMPARISON fit &lt;- glmnet(x, y, intercept=F, standardize=F, lambda = lambda, alpha=alpha) catn(&quot;Glmnet objective (unscaled y)&quot;, objective_value(y, x, coef(fit)[-1], lambda, alpha)) ## Glmnet objective (unscaled y) 2.573793 loss &lt;- sum_squares(y - x %*% beta) / (2 * nrow(x)) obj &lt;- loss + elastic_reg(beta, lambda = lambda / attr(y_s, &quot;scaled:scale&quot;), alpha) prob &lt;- Problem(Minimize(obj)) beta_est &lt;- solve(prob)$getValue(beta) catn(&quot;CVXR objective (unscaled y)&quot;, objective_value(y, x, beta_est, lambda, alpha)) ## CVXR objective (unscaled y) 2.559093 Finally, we can check that the coefficients are close enough. print_matrix(round(cbind(beta_est, coef(fit)[-1]), 3), row_names = sprintf(&quot;$\\\\beta_{%d}$&quot;, seq_len(20)), col_names = c(&quot;CVXR&quot;, &quot;GLMNET&quot;)) CVXR GLMNET \\(\\beta_{1}\\) 1.117 1.123 \\(\\beta_{2}\\) 0.070 0.069 \\(\\beta_{3}\\) 0.568 0.571 \\(\\beta_{4}\\) 0.007 0.008 \\(\\beta_{5}\\) -0.683 -0.687 \\(\\beta_{6}\\) 0.537 0.539 \\(\\beta_{7}\\) 0.121 0.121 \\(\\beta_{8}\\) 0.302 0.304 \\(\\beta_{9}\\) -0.075 -0.074 \\(\\beta_{10}\\) 0.054 0.055 \\(\\beta_{11}\\) 0.143 0.144 \\(\\beta_{12}\\) -0.073 -0.073 \\(\\beta_{13}\\) -0.029 -0.029 \\(\\beta_{14}\\) -0.932 -0.937 \\(\\beta_{15}\\) -0.077 -0.078 \\(\\beta_{16}\\) 0.018 0.018 \\(\\beta_{17}\\) 0.008 0.007 \\(\\beta_{18}\\) 0.009 0.010 \\(\\beta_{19}\\) 0.031 0.031 \\(\\beta_{20}\\) -0.784 -0.790 10.2.2 Exercise Using the data (X, Y) above, solve an elastic net problem with Huber loss using the Huber threshold \\(M = 0.5\\). 10.2.2.1 Solution Just set the loss as follows. beta &lt;- Variable(p) loss &lt;- sum(huber(Y - X %*% beta, M = 0.5)) ## Elastic-net regression LASSO alpha &lt;- 1 beta_vals &lt;- sapply(lambda_vals, function (lambda) { obj &lt;- loss + elastic_penalty(beta, lambda, alpha) prob &lt;- Problem(Minimize(obj)) result &lt;- solve(prob) result$getValue(beta) }) The estimates are below. \\(\\lambda = 0.010\\) \\(\\lambda = 0.026\\) \\(\\lambda = 0.066\\) \\(\\lambda = 0.171\\) \\(\\lambda = 0.441\\) \\(\\lambda = 1.135\\) \\(\\lambda = 2.924\\) \\(\\lambda = 7.533\\) \\(\\lambda = 19.408\\) \\(\\lambda = 50.000\\) \\(\\beta_{1}\\) 0.538 0.538 0.538 0.538 0.536 0.534 0.527 0.497 0.431 0.149 \\(\\beta_{2}\\) -0.371 -0.371 -0.371 -0.371 -0.370 -0.370 -0.369 -0.352 -0.351 -0.147 \\(\\beta_{3}\\) 0.424 0.424 0.424 0.424 0.423 0.422 0.421 0.433 0.391 0.195 \\(\\beta_{4}\\) -0.874 -0.874 -0.874 -0.874 -0.873 -0.873 -0.871 -0.871 -0.798 -0.638 \\(\\beta_{5}\\) -0.129 -0.129 -0.129 -0.129 -0.129 -0.129 -0.130 -0.128 -0.084 0.000 \\(\\beta_{6}\\) 1.233 1.233 1.233 1.233 1.233 1.232 1.230 1.237 1.155 0.828 \\(\\beta_{7}\\) -0.067 -0.067 -0.066 -0.066 -0.066 -0.064 -0.060 -0.023 -0.059 0.000 \\(\\beta_{8}\\) -0.590 -0.590 -0.590 -0.590 -0.589 -0.586 -0.580 -0.559 -0.494 -0.116 \\(\\beta_{9}\\) 1.419 1.419 1.419 1.418 1.417 1.414 1.407 1.377 1.310 0.987 \\(\\beta_{10}\\) -0.266 -0.266 -0.266 -0.266 -0.266 -0.265 -0.265 -0.255 -0.225 -0.069 References "],["nearly-isotonic-fits.html", "Chapter 11 Nearly Isotonic Fits 11.1 Goals 11.2 Global Warming Example 11.3 References", " Chapter 11 Nearly Isotonic Fits 11.1 Goals Formulate nearly-isotonic and nearly-convex fits using CVXR atoms Use the bootstrap to estimate variance of approximation Given a set of data points \\(y \\in {\\mathbf R}^m\\), R. J. Tibshirani, Hoefling, and Tibshirani (2011) fit a nearly-isotonic approximation \\(\\beta \\in {\\mathbf R}^m\\) by solving \\[ \\begin{array}{ll} \\underset{\\beta}{\\mbox{minimize}} &amp; \\frac{1}{2}\\sum_{i=1}^m (y_i - \\beta_i)^2 + \\lambda \\sum_{i=1}^{m-1}(\\beta_i - \\beta_{i+1})_+, \\end{array} \\] where \\(\\lambda \\geq 0\\) is a penalty parameter and \\(x_+ =\\max(x,0)\\). This can be directly formulated in CVXR. 11.2 Global Warming Example As an example, we use global warming data from the Carbon Dioxide Information Analysis Center (CDIAC). The data points are the annual temperature anomalies relative to the 19611990 mean. data(cdiac) str(cdiac) ## &#39;data.frame&#39;: 166 obs. of 14 variables: ## $ year : int 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 ... ## $ jan : num -0.702 -0.303 -0.308 -0.177 -0.36 -0.176 -0.119 -0.512 -0.532 -0.307 ... ## $ feb : num -0.284 -0.362 -0.477 -0.33 -0.28 -0.4 -0.373 -0.344 -0.707 -0.192 ... ## $ mar : num -0.732 -0.485 -0.505 -0.318 -0.284 -0.303 -0.513 -0.434 -0.55 -0.334 ... ## $ apr : num -0.57 -0.445 -0.559 -0.352 -0.349 -0.217 -0.371 -0.646 -0.517 -0.203 ... ## $ may : num -0.325 -0.302 -0.209 -0.268 -0.23 -0.336 -0.119 -0.567 -0.651 -0.31 ... ## $ jun : num -0.213 -0.189 -0.038 -0.179 -0.215 -0.16 -0.288 -0.31 -0.58 -0.25 ... ## $ jul : num -0.128 -0.215 -0.016 -0.059 -0.228 -0.268 -0.297 -0.544 -0.324 -0.285 ... ## $ aug : num -0.233 -0.153 -0.195 -0.148 -0.163 -0.159 -0.305 -0.327 -0.28 -0.104 ... ## $ sep : num -0.444 -0.108 -0.125 -0.409 -0.115 -0.339 -0.459 -0.393 -0.339 -0.575 ... ## $ oct : num -0.452 -0.063 -0.216 -0.359 -0.188 -0.211 -0.384 -0.467 -0.2 -0.255 ... ## $ nov : num -0.19 -0.03 -0.187 -0.256 -0.369 -0.212 -0.608 -0.665 -0.644 -0.316 ... ## $ dec : num -0.268 -0.067 0.083 -0.444 -0.232 -0.51 -0.44 -0.356 -0.3 -0.363 ... ## $ annual: num -0.375 -0.223 -0.224 -0.271 -0.246 -0.271 -0.352 -0.46 -0.466 -0.286 ... Since we plan to fit the regression and also get some idea of the standard errors, we write a function that computes the fit for use in bootstrapping. neariso_fit &lt;- function(y, lambda) { m &lt;- length(y) beta &lt;- Variable(m) A &lt;- as(m:1, &quot;pMatrix&quot;) %*% Matrix::Diagonal(m) # Matrix for reversing order obj &lt;- 0.5 * sum_squares(y - beta) + lambda * sum(pos(diff(A %*% beta))) prob &lt;- Problem(Minimize(obj)) solve(prob)$getValue(beta) } Observe that we reverse the order of beta since diff outputs beta[i+1] - beta[i] for \\(i = 1,\\ldots,m\\). The CVXR::pos atom evaluates \\(x_+ = \\max(x,0)\\) elementwise on the input expression. The boot library provides all the tools for bootstrapping, but requires a statistic function that takes particular arguments: a data frame, followed by the bootstrap indices and any other arguments (\\(\\lambda\\) for instance). This is defined below. NOTE In what follows, we use a very small number of bootstrap samples as the fits are time consuming. neariso_fit_stat &lt;- function(data, index, lambda) { sample &lt;- data[index,] # Bootstrap sample of rows sample &lt;- sample[order(sample$year),] # Order ascending by year neariso_fit(sample$annual, lambda) } set.seed(123) boot.neariso &lt;- boot(data = cdiac, statistic = neariso_fit_stat, R = 10, lambda = 0.44) ci.neariso &lt;- t(sapply(seq_len(nrow(cdiac)), function(i) boot.ci(boot.out = boot.neariso, conf = 0.95, type = &quot;norm&quot;, index = i)$normal[-1])) data.neariso &lt;- data.frame(year = cdiac$year, annual = cdiac$annual, est = boot.neariso$t0, lower = ci.neariso[, 1], upper = ci.neariso[, 2]) We can now plot the fit and confidence bands for the nearly-isotonic fit. (plot.neariso &lt;- ggplot(data = data.neariso) + geom_point(mapping = aes(year, annual), color = &quot;red&quot;) + geom_line(mapping = aes(year, est), color = &quot;blue&quot;) + geom_ribbon(mapping = aes(x = year, ymin = lower,ymax = upper),alpha=0.3) + labs(x = &quot;Year&quot;, y = &quot;Temperature Anomalies&quot;) ) The curve follows the data well, but exhibits some choppiness in regions with a steep trend. 11.2.1 Exercise Fit a smoother curve using a nearly-convex fit described in the same paper: \\[ \\begin{array}{ll} \\underset{\\beta}{\\mbox{minimize}} &amp; \\frac{1}{2}\\sum_{i=1}^m (y_i - \\beta_i)^2 + \\lambda \\sum_{i=1}^{m-2}(\\beta_i - 2\\beta_{i+1} + \\beta_{i+2})_+ \\end{array} \\] 11.2.1.1 Solution This replaces the first difference term with an approximation to the second derivative at \\(\\beta_{i+1}\\). In CVXR, the only change necessary is the penalty line: replace diff(A %*% x) by diff(A %*% x, differences = 2). nearconvex_fit &lt;- function(y, lambda) { m &lt;- length(y) beta &lt;- Variable(m) A &lt;- as(m:1, &quot;pMatrix&quot;) %*% Matrix::Diagonal(m) # Matrix for reversing order obj &lt;- 0.5 * sum_squares(y - beta) + lambda * sum(pos(diff(A %*% beta, differences = 2))) prob &lt;- Problem(Minimize(obj)) solve(prob)$getValue(beta) } nearconvex_fit_stat &lt;- function(data, index, lambda) { sample &lt;- data[index,] # Bootstrap sample of rows sample &lt;- sample[order(sample$year),] # Order ascending by year nearconvex_fit(sample$annual, lambda) } set.seed(987) boot.nearconvex &lt;- boot(data = cdiac, statistic = nearconvex_fit_stat, R = 5, lambda = 0.44) ci.nearconvex &lt;- t(sapply(seq_len(nrow(cdiac)), function(i) boot.ci(boot.out = boot.nearconvex, conf = 0.95, type = &quot;norm&quot;, index = i)$normal[-1])) data.nearconvex &lt;- data.frame(year = cdiac$year, annual = cdiac$annual, est = boot.nearconvex$t0, lower = ci.nearconvex[, 1], upper = ci.nearconvex[, 2]) The resulting curve for the nearly-convex fit is depicted below with 95% confidence bands generated from \\(R = 5\\) samples. Note the jagged staircase pattern has been smoothed out. (plot.nearconvex &lt;- ggplot(data = data.nearconvex) + geom_point(mapping = aes(year, annual), color = &quot;red&quot;) + geom_line(mapping = aes(year, est), color = &quot;blue&quot;) + geom_ribbon(mapping = aes(x = year, ymin = lower,ymax = upper), alpha = 0.3) + labs(x = &quot;Year&quot;, y = &quot;Temperature Anomalies&quot;) ) 11.3 References References "],["speed-considerations.html", "Chapter 12 Speed Considerations", " Chapter 12 Speed Considerations NOTE: The material in this section was written for CVXR pre 1.0. It no longer applies to versions 1.0 and above. Indeed CVXR is actually much faster now by default. We will update this section for 1.0 as appropriate later. "],["pliable-lasso.html", "Chapter 13 Pliable Lasso 13.1 Goals 13.2 Pliable Lasso Problem 13.3 Example 13.4 Full Function 13.5 Results 13.6 Final Comments 13.7 References", " Chapter 13 Pliable Lasso 13.1 Goals Demonstrate how to fit a complex model Show how to apply an atom along a row/column axis 13.2 Pliable Lasso Problem Robert J. Tibshirani and Friedman (2017) propose a generalization of the lasso that allows the model coefficients to vary as a function of a general set of modifying variables, such as gender, age, and time. The pliable lasso model has the form \\[ \\begin{equation} \\hat{y} = \\beta_0{\\mathbf 1} + Z\\theta_0 + \\sum_{j=1}^p(X_j\\beta_j + W_j\\theta_j), \\end{equation} \\] where \\(\\hat{y}\\) is the predicted \\(N\\times1\\) vector, \\(\\beta_0\\) is a scalar, \\(\\theta_0\\) is a \\(K\\)-vector, \\(X\\) and \\(Z\\) are \\(N\\times p\\) and \\(N\\times K\\) matrices containing values of the predictor and modifying variables, respectively, and \\(W_j=X_j \\circ Z\\) denotes the elementwise multiplication of \\(Z\\) by column \\(X_j\\) of \\(X\\). The objective function used for pliable lasso is \\[ J(\\beta_0, \\theta_0, \\beta, \\Theta) = \\frac{1}{2N}\\sum_{i=1}^N (y_i-\\hat{y}_i)^2 + (1-\\alpha)\\lambda\\sum_{j=1}^p\\biggl(||(\\beta_j,\\theta_j)||_2 + ||\\theta_j||_2\\biggr) + \\alpha\\lambda\\sum_{j,k}|\\theta_{j,k}|_1. \\] In the above, \\(\\Theta\\) is a \\(p\\times K\\) matrix of parameters with \\(j\\)-th row \\(\\theta_j\\) and individual entries \\(\\theta_{j,k}\\), and \\(\\alpha\\) and \\(\\lambda\\) are tuning parameters. As \\(\\alpha \\rightarrow 1\\) (but \\(&lt;1\\)), the solution approaches the lasso solution. The default value used in the paper is \\(\\alpha = 0.5.\\) An R package for the pliable lasso is currently archived on CRAN. Nevertheless, the pliable lasso is an excellent example to highlight the prototyping capabilities of CVXR for research. Along the way, we also introduce some new atoms that are needed in this example. 13.3 Example We will use a simulated example from Section 3 of Robert J. Tibshirani and Friedman (2017) with \\(n=100\\), \\(p=50\\) and \\(K=4\\). The response is generated as \\[ \\begin{eqnarray*} y &amp;=&amp; \\mu(x) + 0.5\\epsilon;\\ \\ \\epsilon \\sim N(0, 1)\\\\ \\mu(x) &amp;=&amp; x_1\\beta_1 + x_2\\beta_2 + x_3(\\beta_3 e + 2z_1) + x_4\\beta_4(e - 2z_2);\\ \\ \\beta = (2, -2, 2, 2, 0, 0, \\ldots), \\end{eqnarray*} \\] where \\(e=(1,1,\\ldots ,1)^T\\). ## Simulation data. set.seed(123) N &lt;- 100 K &lt;- 4 p &lt;- 50 X &lt;- matrix(rnorm(n = N * p, mean = 0, sd = 1), nrow = N, ncol = p) Z &lt;- matrix(rbinom(n = N * K, size = 1, prob = 0.5), nrow = N, ncol = K) ## Response model. beta &lt;- rep(x = 0, times = p) beta[1:4] &lt;- c(2, -2, 2, 2) coeffs &lt;- cbind(beta[1], beta[2], beta[3] + 2 * Z[, 1], beta[4] * (1 - 2 * Z[, 2])) mu &lt;- diag(X[, 1:4] %*% t(coeffs)) y &lt;- mu + 0.5 * rnorm(N, mean = 0, sd = 1) It seems worthwhile to write a function that will fit the model for us so that we can customize a few things such as the intercept term, verbosity, etc. The function has the following structure with comments as placeholders for code we shall construct later. plasso_fit &lt;- function(y, X, Z, lambda, alpha = 0.5, intercept = TRUE, ZERO_THRESHOLD= 1e-6, verbose = FALSE) { N &lt;- length(y) p &lt;- ncol(X) K &lt;- ncol(Z) beta0 &lt;- 0 if (intercept) { beta0 &lt;- Variable(1) * matrix(1, nrow = N, ncol = 1) } ## Define_Parameters ## Build_Penalty_Terms ## Compute_Fitted_Value ## Build_Objective ## Define_and_Solve_Problem ## Return_Values } ## Fit pliable lasso using CVXR. # pliable &lt;- pliable_lasso(y, X, Z, alpha = 0.5, lambda = lambda) 13.3.1 Parameters The parameters are easy: we just have \\(\\beta\\), \\(\\theta_0\\) and \\(\\Theta\\). beta &lt;- Variable(p) theta0 &lt;- Variable(K) theta &lt;- Variable(p, K); theta_transpose &lt;- t(theta) Note that we also define the transpose of \\(\\Theta\\) for use later. 13.3.2 Penalty Terms There are three of them. The first term in the parentheses, \\(\\sum_{j=1}^p\\biggl(||(\\beta_j,\\theta_j)||_2\\biggr)\\), involves components of \\(\\beta\\) and rows of \\(\\Theta\\). CVXR provides two functions to express this norm: hstack to bind columns of \\(\\beta\\) and the matrix \\(\\Theta\\), the equivalent of cbind in R, cvxr_norm, which accepts a matrix variable and an axis denoting the axis along which the norm is to be taken. The penalty requires us to use the row axis, so axis = 1 per the usual R convention. The second term in the parentheses, \\(\\sum_{j=1}^p||\\theta_j||_2\\), is also a norm along rows as the \\(\\theta_j\\) are rows of \\(\\Theta\\). The last term is simply a \\(l_1\\)-norm. penalty_term1 &lt;- sum(cvxr_norm(hstack(beta, theta), 2, axis = 1)) penalty_term2 &lt;- sum(cvxr_norm(theta, 2, axis = 1)) penalty_term3 &lt;- sum(cvxr_norm(theta, 1)) 13.3.3 Fitted Value Equation 1 above for \\(\\hat{y}\\) contains a sum: \\(\\sum_{j=1}^p(X_j\\beta_j + W_j\\theta_j)\\). This requires multiplication of \\(Z\\) by the columns of \\(X\\) component-wise and is thus a natural candidate for a map-reduce combination: map the column multiplication function appropriately and reduce using + to obtain the XZ_term below. xz_theta &lt;- lapply(seq_len(p), function(j) (matrix(X[, j], nrow = N, ncol = K) * Z) %*% theta_transpose[, j]) XZ_term &lt;- Reduce(f = &#39;+&#39;, x = xz_theta) y_hat &lt;- beta0 + X %*% beta + Z %*% theta0 + XZ_term 13.3.4 Objective Function Building the objective is now straightforward. objective &lt;- sum_squares(y - y_hat) / (2 * N) + (1 - alpha) * lambda * (penalty_term1 + penalty_term2) + alpha * lambda * penalty_term3 13.3.5 Solving the Problem prob &lt;- Problem(Minimize(objective)) result &lt;- solve(prob, verbose = verbose) beta_hat &lt;- result$getValue(beta) 13.3.6 Return Values We create a list with values of interest to us. However, since sparsity is desired, we set values below ZERO_THRESHOLD to zero. theta0_hat &lt;- result$getValue(theta0) theta_hat &lt;- result$getValue(theta) ## Zero out stuff before returning beta_hat[abs(beta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 theta0_hat[abs(theta0_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 theta_hat[abs(theta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 list(beta0_hat = if (intercept) result$getValue(beta0)[1] else 0.0, beta_hat = beta_hat, theta0_hat = theta0_hat, theta_hat = theta_hat, criterion = result$value) 13.4 Full Function We now put it all together. plasso_fit &lt;- function(y, X, Z, lambda, alpha = 0.5, intercept = TRUE, ZERO_THRESHOLD= 1e-6, verbose = FALSE) { N &lt;- length(y) p &lt;- ncol(X) K &lt;- ncol(Z) beta0 &lt;- 0 if (intercept) { beta0 &lt;- Variable(1) * matrix(1, nrow = N, ncol = 1) } beta &lt;- Variable(p) theta0 &lt;- Variable(K) theta &lt;- Variable(p, K); theta_transpose &lt;- t(theta) penalty_term1 &lt;- sum(cvxr_norm(hstack(beta, theta), 2, axis = 1)) penalty_term2 &lt;- sum(cvxr_norm(theta, 2, axis = 1)) penalty_term3 &lt;- sum(cvxr_norm(theta, 1)) xz_theta &lt;- lapply(seq_len(p), function(j) (matrix(X[, j], nrow = N, ncol = K) * Z) %*% theta_transpose[, j]) XZ_term &lt;- Reduce(f = &#39;+&#39;, x = xz_theta) y_hat &lt;- beta0 + X %*% beta + Z %*% theta0 + XZ_term objective &lt;- sum_squares(y - y_hat) / (2 * N) + (1 - alpha) * lambda * (penalty_term1 + penalty_term2) + alpha * lambda * penalty_term3 prob &lt;- Problem(Minimize(objective)) result &lt;- solve(prob, verbose = verbose) beta_hat &lt;- result$getValue(beta) theta0_hat &lt;- result$getValue(theta0) theta_hat &lt;- result$getValue(theta) ## Zero out stuff before returning beta_hat[abs(beta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 theta0_hat[abs(theta0_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 theta_hat[abs(theta_hat) &lt; ZERO_THRESHOLD] &lt;- 0.0 list(beta0_hat = if (intercept) result$getValue(beta0)[1] else 0.0, beta_hat = beta_hat, theta0_hat = theta0_hat, theta_hat = theta_hat, criterion = result$value) } 13.5 Results Using \\(\\lambda = 0.6\\), we fit the pliable lasso without an intercept. result &lt;- plasso_fit(y, X, Z, lambda = 0.6, alpha = 0.5, intercept = FALSE) We can print the various estimates. cat(sprintf(&quot;Objective value: %f\\n&quot;, result$criterion)) ## Objective value: 4.153289 We only print the nonzero \\(\\beta\\) values. index &lt;- which(result$beta_hat != 0) est.table &lt;- data.frame(matrix(result$beta_hat[index], nrow = 1)) names(est.table) &lt;- paste0(&quot;$\\\\beta_{&quot;, index, &quot;}$&quot;) knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;% kable_styling(&quot;striped&quot;) \\(\\beta_{1}\\) \\(\\beta_{2}\\) \\(\\beta_{3}\\) \\(\\beta_{4}\\) \\(\\beta_{20}\\) 1.716 -1.385 2.217 0.285 -0.038 For this value of \\(\\lambda\\), the nonzero \\((\\beta_1, \\beta_2, \\beta_3,\\beta_4)\\) are picked up along with an additional \\(\\beta_{20}\\). The values for \\(\\theta_0\\) are given below. est.table &lt;- data.frame(matrix(result$theta0_hat, nrow = 1)) names(est.table) &lt;- paste0(&quot;$\\\\theta_{0,&quot;, 1:K, &quot;}$&quot;) knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;% kable_styling(&quot;striped&quot;) \\(\\theta_{0,1}\\) \\(\\theta_{0,2}\\) \\(\\theta_{0,3}\\) \\(\\theta_{0,4}\\) -0.136 0.25 -0.516 0.068 Finally, we display just the first five rows of \\(\\Theta\\), which happen to contain all the nonzero values for this result. est.table &lt;- data.frame(result$theta_hat[1:5, ]) names(est.table) &lt;- paste0(&quot;$\\\\theta_{,&quot;, 1:K, &quot;}$&quot;) knitr::kable(est.table, format = &quot;html&quot;, digits = 3) %&gt;% kable_styling(&quot;striped&quot;) \\(\\theta_{,1}\\) \\(\\theta_{,2}\\) \\(\\theta_{,3}\\) \\(\\theta_{,4}\\) 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.528 0.000 0.085 0.308 0.063 -0.592 -0.019 -0.185 0.000 0.000 0.000 0.000 13.6 Final Comments Typically, one would run the fits for various values of \\(\\lambda\\), choose one based on cross-validation, and assess the prediction against a test set. Here, even a single fit takes a while. However, the potential of CVXR in prototyping novel methods is clear. 13.7 References References "],["survey-calibration.html", "Chapter 14 Survey Calibration 14.1 Goals 14.2 Survey Calibration Problem 14.3 Raking 14.4 References", " Chapter 14 Survey Calibration 14.1 Goals Demonstrate different methods for calibrating data 14.2 Survey Calibration Problem Calibration is a widely used technique in survey sampling. Suppose \\(m\\) sampling units in a survey have been assigned initial weights \\(d_i\\) for \\(i = 1,\\ldots,m\\), and furthermore, there are \\(n\\) auxiliary variables whose values in the sample are known. Calibration seeks to improve the initial weights \\(d_i\\) by finding new weights \\(w_i\\) that incorporate this auxiliary information while perturbing the initial weights as little as possible, , the ratio \\(g_i = w_i/d_i\\) must be close to one. Such reweighting improves precision of estimates (Chapter 7, T. S. Lumley (2010)). Let \\(X \\in {\\mathbf R}^{m \\times n}\\) be the matrix of survey samples, with each column corresponding to an auxiliary variable. Reweighting can be expressed as the optimization problem (see Davies, Gillard, and Zhigljavsky (2016)): \\[ \\begin{array}{ll} \\mbox{minimize} &amp; \\sum_{i=1}^m d_i\\phi(g_i) \\\\ \\mbox{subject to} &amp; A^Tg = r \\end{array} \\] with respect to \\(g \\in {\\mathbf R}^m\\), where \\(\\phi:{\\mathbf R} \\rightarrow {\\mathbf R}\\) is a strictly convex function with \\(\\phi(1) = 0\\), \\(r \\in {\\mathbf R}^n\\) are the known population totals of the auxiliary variables, and \\(A \\in {\\mathbf R}^{m \\times n}\\) is related to \\(X\\) by \\(A_{ij} = d_iX_{ij}\\) for \\(i = 1,\\ldots,m\\) and \\(j = 1,\\ldots,n\\). 14.3 Raking A common calibration technique is raking, which uses the penalty function \\(\\phi(g_i) = g_i\\log(g_i) - g_i + 1\\) as the calibration metric. We illustrate this with the California Academic Performance Index data in the survey package (T. Lumley (2018)), which also supplies facilities for calibration via the function calibrate. Both the population dataset (apipop) and a simple random sample of \\(m = 200\\) (apisrs) are provided. Suppose that we wish to reweight the observations in the sample using known totals for two variables from the population: stype, the school type (elementary, middle, or high) and sch.wide, whether the school met the yearly target or not. This reweighting would make the sample more representative of the general population. The code below estimates the weights using survey::calibrate. data(api) design_api &lt;- svydesign(id = ~dnum, weights = ~pw, data = apisrs) formula &lt;- ~stype + sch.wide T &lt;- apply(model.matrix(object = formula, data = apipop), 2, sum) cal_api &lt;- calibrate(design_api, formula, population = T, calfun = cal.raking) w_survey &lt;- weights(cal_api) The CVXR formulation follows. di &lt;- apisrs$pw X &lt;- model.matrix(object = formula, data = apisrs) A &lt;- di * X n &lt;- nrow(apisrs) g &lt;- Variable(n) constraints &lt;- list(t(A) %*% g == T) ## Raking Phi_R &lt;- Minimize(sum(di * (-entr(g) - g + 1))) p &lt;- Problem(Phi_R, constraints) res &lt;- solve(p) w_cvxr &lt;- di * res$getValue(g) The results are identical as shown in the table below. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. Table 14.1: Calibration weights from Raking stype sch.wide Survey wts. Frequency CVXR wts. Frequency E No 28.911 15 28.911 15 E Yes 31.396 127 31.396 127 H No 29.003 13 29.003 13 H Yes 31.497 12 31.497 12 M No 29.033 9 29.033 9 M Yes 31.529 24 31.529 24 14.3.1 Exercise The quadratic penalty function \\[ \\phi^{Q}(g) = \\frac{1}{2}(g-1)^2 \\] is also sometimes used. Calibrate using CVXR and the SCS solver and compare with the survey::cal.linear results, which can be obtained via w_survey_q &lt;- weights(calibrate(design_api, formula, population = T, calfun = cal.linear)) 14.3.1.1 Solution ## Quadratic Phi_Q &lt;- Minimize(sum_squares(g - 1) / 2) p &lt;- Problem(Phi_Q, constraints) res &lt;- solve(p, solver = &quot;SCS&quot;) w_cvxr_q &lt;- di * res$getValue(g) The default OSQP solver produces a different number of unique weights. Such differences are not unheard of among solvers. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. Table 14.2: Calibration weights from Quadratic metric stype sch.wide Survey wts. Frequency CVXR wts. Frequency E No 28.907 15 30.063 15 E Yes 31.397 127 32.653 127 H No 29.005 13 30.165 13 H Yes 31.495 12 32.755 12 M No 29.037 9 30.199 9 M Yes 31.528 24 32.789 24 14.3.2 Exercise Repeat the above exercise with the logistic function \\[ \\phi^{L}(g; l, u) = \\frac{1}{C}\\biggl[ (g-l)\\log\\left(\\frac{g-l}{1-l}\\right) + (u-g)\\log\\left(\\frac{u-g}{u-1}\\right) \\biggr] \\mbox{ for } C = \\frac{u-l}{(1-l)(u-1)}, \\] which requires bounds \\(l\\) and \\(u\\) on the coefficients. Use \\(l=0.9\\) and \\(u=1.1\\). The results from survey::cal.linear can be obtained with the code below. u &lt;- 1.10; l &lt;- 0.90 w_survey_l &lt;- weights(calibrate(design_api, formula, population = T, calfun = cal.linear, bounds = c(l, u))) 14.3.3 Solution Phi_L &lt;- Minimize(sum(-entr((g - l) / (u - l)) - entr((u - g) / (u - l)))) p &lt;- Problem(Phi_L, c(constraints, list(l &lt;= g, g &lt;= u))) res &lt;- solve(p) w_cvxr_l &lt;- di * res$getValue(g) ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. Table 14.3: Calibration weights from Logit metric stype sch.wide Survey wts. Frequency CVXR wts. Frequency E No 28.907 15 28.929 15 E Yes 31.397 127 31.394 127 H No 29.005 13 28.995 13 H Yes 31.495 12 31.505 12 M No 29.037 9 29.014 9 M Yes 31.528 24 31.536 24 14.3.4 Exercise Repeat with the Hellinger distance \\[ \\Phi^{H}(g) = \\frac{1}{(1 - g/2)^2} \\] and the following results. hellinger &lt;- make.calfun(Fm1 = function(u, bounds) ((1 - u / 2)^-2) - 1, dF= function(u, bounds) (1 -u / 2)^-3 , name = &quot;Hellinger distance&quot;) w_survey_h &lt;- weights(calibrate(design_api, formula, population = T, calfun = hellinger)) 14.3.4.1 Solution Phi_h &lt;- Minimize(sum((1 - g / 2)^(-2))) p &lt;- Problem(Phi_h, constraints) res &lt;- solve(p) w_cvxr_h &lt;- di * res$getValue(g) ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. Table 14.4: Calibration weights from Hellinger distance metric stype sch.wide Survey wts. Frequency CVXR wts. Frequency E No 28.913 15 28.890 15 E Yes 31.396 127 31.399 127 H No 29.002 13 29.011 13 H Yes 31.498 12 31.488 12 M No 29.031 9 29.056 9 M Yes 31.530 24 31.521 24 14.3.5 Exercise Lastly, use the derivative of the inverse hyperbolic sine \\[ \\Phi^{S}(g) = \\frac{1}{2}(e^g + e^{-g}) \\] along with the results produced below. w_survey_s &lt;- weights(calibrate(design_api, formula, population = T, calfun = cal.sinh, bounds = c(l, u))) 14.3.5.1 Solution Phi_s &lt;- Minimize(sum( 0.5 * (exp(g) + exp(-g)))) p &lt;- Problem(Phi_s, c(constraints, list(l &lt;= g, g &lt;= u))) res &lt;- solve(p) w_cvxr_s &lt;- di * res$getValue(g) ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;stype&#39;. You can override using the `.groups` argument. Table 14.5: Calibration weights from derivative of sinh metric stype sch.wide Survey wts. Frequency CVXR wts. Frequency E No 28.911 15 28.904 15 E Yes 31.396 127 31.397 127 H No 29.003 13 29.006 13 H Yes 31.497 12 31.494 12 M No 29.033 9 29.041 9 M Yes 31.529 24 31.526 24 14.4 References References "],["sparse-inverse-covariance-estimation.html", "Chapter 15 Sparse Inverse Covariance Estimation 15.1 Goals 15.2 Sparse Inverse Covariance Problem 15.3 Example 15.4 Results 15.5 References", " Chapter 15 Sparse Inverse Covariance Estimation 15.1 Goals Introduce positive semidefinite matrices and CVXR::log_det Demonstrate how to build functions for complex tasks 15.2 Sparse Inverse Covariance Problem Assume we are given i.i.d. observations \\(x_i \\sim N(0,\\Sigma)\\) for \\(i = 1,\\ldots,m\\), where the covariance matrix \\(\\Sigma \\in {\\mathbf S}_+^n\\), the set of symmetric positive semidefinite matrices, has a sparse inverse \\(S = \\Sigma^{-1}\\). Let \\(Q = \\frac{1}{m-1}\\sum_{i=1}^m (x_i - \\bar x)(x_i - \\bar x)^T\\) be our sample covariance. One way to estimate \\(\\Sigma\\) is to maximize the log-likelihood with the prior knowledge that \\(S\\) is sparse (Friedman, Hastie, and Tibshirani 2008), which amounts to the optimization problem: \\[ \\begin{array}{ll} \\underset{S}{\\mbox{maximize}} &amp; \\log\\det(S) - \\mbox{tr}(SQ) \\\\ \\mbox{subject to} &amp; \\sum_{i=1}^n \\sum_{j=1}^n |S_{ij}| \\leq \\alpha \\end{array} \\] with respect to \\(S \\in {\\mathbf S}_+^n\\), where the parameter \\(\\alpha \\geq 0\\) controls the degree of sparsity. The problem is convex, so we can solve it using CVXR. 15.3 Example Well create a sparse positive semidefinite matrix \\(S\\) using synthetic data. set.seed(1) n &lt;- 10 ## Dimension of matrix m &lt;- 1000 ## Number of samples ## Create sparse, symmetric PSD matrix S A &lt;- rsparsematrix(n, n, 0.15, rand.x = stats::rnorm) Strue &lt;- A %*% t(A) + 0.05 * diag(rep(1, n)) ## Force matrix to be strictly positive definite We then invert \\(S\\) to get \\(R = S^{-1}\\). R &lt;- base::solve(Strue) As test data, we sample from a multivariate normal distribution using the fact that if \\(Y \\sim N(0, I)\\), then \\(R^{1/2}Y \\sim N(0, R)\\) since \\(R\\) is symmetric. x_sample &lt;- matrix(stats::rnorm(n * m), nrow = m, ncol = n) %*% t(expm::sqrtm(R)) Q &lt;- cov(x_sample) ## Sample covariance matrix Finally, we solve our convex program for a range of \\(\\alpha\\) values. alphas &lt;- c(10, 8, 6, 4, 1) S &lt;- Variable(c(n, n), PSD = TRUE) ## Variable constrained to positive semidefinite cone obj &lt;- Maximize(log_det(S) - matrix_trace(S %*% Q)) S.est &lt;- lapply(alphas, function(alpha) { constraints &lt;- list(sum(abs(S)) &lt;= alpha) ## Form and solve optimization problem prob &lt;- Problem(obj, constraints) result &lt;- solve(prob) ## Create covariance matrix R_hat &lt;- base::solve(result$getValue(S)) Sres &lt;- result$getValue(S) Sres[abs(Sres) &lt;= 1e-4] &lt;- 0 Sres }) In the code above, the PSD = TRUE argument to the Variable constructor restricts S to the positive semidefinite cone. We use CVXR functions for the log-determinant and trace in the objective. The expression matrix_trace(S %*% Q) is equivalent to sum(diag(S %*% Q))}, but the former is preferred because it is more efficient than making nested function calls. However, a standalone atom does not exist for the determinant, so we cannot replace log_det(S) with log(det(S)) since det is undefined for a Variable object. 15.4 Results The figures below depict the solutions for the dataset with \\(m = 1000, n = 10\\), and \\(S\\) containing 26% non-zero entries, represented by the dark squares in the images below. The sparsity of our inverse covariance estimate decreases for higher \\(\\alpha\\), so that when \\(\\alpha = 1\\), most of the off-diagonal entries are zero, while if \\(\\alpha = 10\\), over half the matrix is dense. At \\(\\alpha = 4\\), we achieve the approximate true percentage of non-zeros. do.call(multiplot, args = c(list(plotSpMat(Strue)), mapply(plotSpMat, S.est, alphas, SIMPLIFY = FALSE), list(layout = matrix(1:6, nrow = 2, byrow = TRUE)))) 15.5 References References "],["portfolio-estimation.html", "Chapter 16 Portfolio Estimation 16.1 Goals 16.2 Markowitz Portfolio Problem 16.3 Example 16.4 Discussion 16.5 References", " Chapter 16 Portfolio Estimation 16.1 Goals Demonstrate CVXR on a financial example 16.2 Markowitz Portfolio Problem The Markowitz portfolio problem (Markowitz 1952; Roy 1952; Lobo, Fazel, and Boyd 2007) is well known in finance. We will solve this problem under various constraints. We have \\(n\\) assets or stocks in our portfolio and must determine the amount of money to invest in each. Let \\(w_i\\) denote the fraction of our budget invested in asset \\(i = 1,\\ldots,m\\), and let \\(r_i\\) be the returns (i.e., fractional change in price) over the period of interest. We model returns as a random vector \\(r \\in {\\mathbf R}^n\\) with known mean \\(\\mathbf{E}[r] = \\mu\\) and covariance \\(\\mathbf{Var}(r) = \\Sigma\\). Thus, given a portfolio \\(w \\in {\\mathbf R}^n\\), the overall return is \\(R = r^Tw\\). Portfolio optimization involves a trade-off between the expected return \\(\\mathbf{E}[R] = \\mu^Tw\\) and the associated risk, which we take to be the return variance \\(\\mathbf{Var}(R) = w^T\\Sigma w\\). Initially, we consider only long portfolios, so our problem is \\[ \\begin{array}{ll} \\underset{w}{\\mbox{maximize}} &amp; \\mu^Tw - \\gamma w^T\\Sigma w \\\\ \\mbox{subject to} &amp; w \\geq 0, \\quad \\sum_{i=1}^n w = 1, \\end{array} \\] where the objective is the risk-adjusted return and \\(\\gamma &gt; 0\\) is a risk aversion parameter. 16.3 Example We construct the risk-return trade-off curve for \\(n = 10\\) assets and \\(\\mu\\) and \\(\\Sigma^{1/2}\\) drawn from a standard normal distribution. ## Problem data set.seed(10) n &lt;- 10 SAMPLES &lt;- 100 mu &lt;- matrix(abs(rnorm(n)), nrow = n) Sigma &lt;- matrix(rnorm(n^2), nrow = n, ncol = n) Sigma &lt;- t(Sigma) %*% Sigma ## Form problem w &lt;- Variable(n) ret &lt;- t(mu) %*% w risk &lt;- quad_form(w, Sigma) constraints &lt;- list(w &gt;= 0, sum(w) == 1) ## Risk aversion parameters gammas &lt;- 10^seq(-2, 3, length.out = SAMPLES) ret_data &lt;- rep(0, SAMPLES) risk_data &lt;- rep(0, SAMPLES) w_data &lt;- matrix(0, nrow = SAMPLES, ncol = n) ## Compute trade-off curve for(i in seq_along(gammas)) { gamma &lt;- gammas[i] objective &lt;- ret - gamma * risk prob &lt;- Problem(Maximize(objective), constraints) result &lt;- solve(prob) ## Evaluate risk/return for current solution risk_data[i] &lt;- result$getValue(sqrt(risk)) ret_data[i] &lt;- result$getValue(ret) w_data[i,] &lt;- result$getValue(w) } Note how we obtain the risk and return by directly evaluating the value of the separate expressions: result$getValue(risk) result$getValue(ret) The trade-off curve is shown below. The \\(x\\)-axis represents the standard deviation of the return. Red points indicate the result from investing the entire budget in a single asset. As \\(\\gamma\\) increases, our portfolio becomes more diverse, reducing risk but also yielding a lower return. cbPalette &lt;- brewer.pal(n = 10, name = &quot;Paired&quot;) p1 &lt;- ggplot() + geom_line(mapping = aes(x = risk_data, y = ret_data), color = &quot;blue&quot;) + geom_point(mapping = aes(x = sqrt(diag(Sigma)), y = mu), color = &quot;red&quot;) markers_on &lt;- c(10, 20, 30, 40) nstr &lt;- sprintf(&quot;gamma == %.2f&quot;, gammas[markers_on]) df &lt;- data.frame(markers = markers_on, x = risk_data[markers_on], y = ret_data[markers_on], labels = nstr) p1 + geom_point(data = df, mapping = aes(x = x, y = y), color = &quot;black&quot;) + annotate(&quot;text&quot;, x = df$x + 0.2, y = df$y - 0.05, label = df$labels, parse = TRUE) + labs(x = &quot;Risk (Standard Deviation)&quot;, y = &quot;Return&quot;) We can also plot the fraction of budget invested in each asset. w_df &lt;- data.frame(paste0(&quot;grp&quot;, seq_len(ncol(w_data))), t(w_data[markers_on,])) names(w_df) &lt;- c(&quot;grp&quot;, sprintf(&quot;gamma == %.2f&quot;, gammas[markers_on])) tidyW &lt;- gather(w_df, key = &quot;gamma&quot;, value = &quot;fraction&quot;, names(w_df)[-1], factor_key = TRUE) ggplot(data = tidyW, mapping = aes(x = gamma, y = fraction)) + geom_bar(mapping = aes(fill = grp), stat = &quot;identity&quot;) + scale_x_discrete(labels = parse(text = levels(tidyW$gamma))) + scale_fill_manual(values = cbPalette) + guides(fill = &quot;none&quot;) + labs(x = &quot;Risk Aversion&quot;, y = &quot;Fraction of Budget&quot;) 16.4 Discussion Many variations on the classical portfolio problem exist. For instance, we could allow long and short positions, but impose a leverage limit \\(\\|w\\|_1 \\leq L^{max}\\) by changing constr &lt;- list(p_norm(w,1) &lt;= Lmax, sum(w) == 1) An alternative is to set a lower bound on the return and minimize just the risk. To account for transaction costs, we could add a term to the objective that penalizes deviations of \\(w\\) from the previous portfolio. These extensions and more are described in Boyd et al. (2017). The key takeaway is that all of these convex problems can be easily solved in CVXR with just a few alterations to the code above. 16.5 References References "],["using-other-solvers.html", "Chapter 17 Using Other Solvers 17.1 Goals 17.2 Default Solvers 17.3 Commercial Solvers 17.4 Solver Peculiarities 17.5 References", " Chapter 17 Using Other Solvers 17.1 Goals Show how to use non-default solvers like MOSEK and GUROBI Discuss solver peculiarities 17.2 Default Solvers The default installation of CVXR comes with three (imported) open source solvers: ECOS and its mixed integer cousin ECOS_BB via the CRAN package ECOSolveR SCS via the CRAN package scs. OSQP via the CRAN package osqp. CVXR can also make use of several other open source solvers implemented in R packages. The linear and mixed integer programming package lpSolve via the lpSolveAPI package The linear and mixed integer programming package GLPK via the Rglpk package. Since these are optional, you must install the packages yourself. lapply(list(LPSOLVE = &quot;lpSolveAPI&quot;, GLPK = &quot;Rglpk&quot;), function(x) x %in% installed.packages()[, 1]) ## $LPSOLVE ## [1] TRUE ## ## $GLPK ## [1] TRUE Once the packages are installed, a call to installed_solvers will display the solvers that CVXR has detected. 17.3 Commercial Solvers A few commercial solvers are also currently supported: MOSEK and GUROBI. CVXR version 1.0 directly calls these solvers via their associated R packages using problem reductions. (Previous CVXR versions required Python and the reticulate package). 17.3.1 Installing MOSEK MOSEK provides an academic version that is free of charge. As noted in the downloads page, Anaconda users can install merely via: conda install -c mosek mosek Others can use the pip command: pip install -f https://download.mosek.com/stable/wheel/index.html Mosek In addition, the license for the product has to be activated per instructions on the Sales section of the MOSEK web page. The Rmosek R package must be installed separately. Once activated, you can check that CVXR recognizes the solver; installed_solvers() should list MOSEK. Otherwise, rinse and repeat until success. 17.3.2 Installing GUROBI GUROBI also provides an academic version that is free of charge. You must register to receive the license. Once registered, install the Gurobi Optimizer software and activate your license as necessary. The gurobi R package must be installed separately. After activation, you can check that CVXR::installed_solvers() lists GUROBI. Otherwise, rinse and repeat until success. 17.3.3 Gotchas If you have an Anaconda installation in your path, you have to account for the fact that there may be interactions when using RStudio and rendering documents. In particular, Anaconda may include its own version of pandoc and other tools that may conflict with what RStudio needs to work properly. To be concrete, one problem we found was that the MOSEK solver was not recognized as available in this rendered document, even though the command line interface showed it to be present. Ensuring an appropriate PATH variable solves the problem. 17.3.4 Example Session installed_solvers() ## [1] &quot;ECOS&quot; &quot;ECOS_BB&quot; &quot;GLPK_MI&quot; &quot;GLPK&quot; &quot;SCS&quot; &quot;GUROBI&quot; &quot;MOSEK&quot; &quot;OSQP&quot; 17.4 Solver Peculiarities The most commonly used solver in CVXR is ECOS. However, it is not always the best solver for a problem. As an example, let us consider again the catenary problem. We will change the problem slightly to use a finer discretization from 101 to 501 points. ## Problem data m &lt;- 501 L &lt;- 2 h &lt;- L / (m - 1) ## Form objective x &lt;- Variable(m) y &lt;- Variable(m) objective &lt;- Minimize(sum(y)) ## Form constraints constraints &lt;- list(x[1] == 0, y[1] == 1, x[m] == 1, y[m] == 1, diff(x)^2 + diff(y)^2 &lt;= h^2) ## Solve the catenary problem prob &lt;- Problem(objective, constraints) result &lt;- solve(prob, solver = &quot;ECOS&quot;) The solution status is no longer optimal. cat(&quot;Solution status is&quot;, result$status) ## Solution status is optimal_inaccurate In such cases, using a different solver may give more accurate results. Let us try MOSEK. if (&quot;MOSEK&quot; %in% installed_solvers()) { result &lt;- solve(prob, solver = &quot;MOSEK&quot;) cat(&quot;Solution status is&quot;, result$status) } else { cat(&quot;Solution not available as MOSEK is not installed!&quot;) } ## Solution status is optimal This returns an optimal solution. Here again, even commercial solvers differ; GUROBI, for example, does not completely solve the problem and in fact throws an error. 17.5 References "],["the-structure-of-atoms.html", "Chapter 18 The Structure of Atoms 18.1 Definition 18.2 Atom Properties 18.3 Canonicalization 18.4 Putting It All Together", " Chapter 18 The Structure of Atoms CVXR comes with a rich library of atoms that represent common functions in convex analysis. When combined using the DCP rules, these atoms are sufficient to model and solve most convex optimization problems, and we encourage most users to work with the CVXR library. However, it is possible for a sophisticated user to add a new atom. To do this requires an understanding of mathematical programming and the S4 class system. In this guide, we go over the basics of implementing an atom, the quadratic-over-linear function, which should provide a starting point for potential developers. 18.1 Definition For any \\(x \\in \\mathbf{R}^n\\) and \\(y \\in \\mathbf{R}\\), the quadratic-over-linear (QoL) function is \\[ f(x,y) := \\frac{\\|x\\|_2^2}{y} \\quad \\mbox{with} \\quad \\mathbf{dom}\\;f = \\{(x,y) \\in \\mathbf{R}^n \\times \\mathbf{R}: y &gt; 0\\}. \\] In CVXR, atoms like this (along with variables, constraints, etc) are represented by S4 class objects. S4 allows us to overload standard mathematical operations so CVXR combines seamlessly with native R script and other packages. The class for the QoL function is QuadOverLin, defined as setClass(&quot;QuadOverLin&quot;, representation(x = &quot;ConstValORExpr&quot;, y = &quot;ConstValORExpr&quot;), contains = &quot;Atom&quot;) We also provide a constructor quad_over_lin &lt;- function(x, y) { new(&quot;QuadOverLin&quot;, x = x, y = y) } The QuadOverLin class inherits from the Atom superclass. It takes as input two arguments, x and y, which must be R numeric constants or CVXR Expression objects. These input types are encapsulated in the ConstaValORExpr class union. Since Expressions themselves may contain other atoms, this allows us to use the QoL function in nested compositions. During initialization, x and y must be passed to the Atom superclass for processing and validation. Consequently, we override the default QuadOverLin initialization method with setMethod(&quot;initialize&quot;, &quot;QuadOverLin&quot;, function(.Object, ..., x, y) { .Object@x &lt;- x .Object@y &lt;- y callNextMethod(.Object, ..., args = list(.Object@x, .Object@y)) }) The first two lines save x and y to their respective slots in QuadOverLin, while the third calls Atoms initialize with args equal to the list of arguments. For explanatory purposes, we reproduce this method below. setMethod(&quot;initialize&quot;, &quot;Atom&quot;, function(.Object, ..., args = list(), .size = NA_real_) { .Object@args &lt;- lapply(args, as.Constant) validate_args(.Object) .Object@.size &lt;- size_from_args(.Object) callNextMethod(.Object, ...) }) It is not important to understand the details of the above code. Only recognize that the atom arguments are converted into CVXR objects and saved in a list in the args slot. We will be accessing this slot later. 18.2 Atom Properties Now that we have created the atoms S4 class, it is time to define methods that characterize its mathematical properties. 18.2.1 Mathematical Basics First, we provide a method for validating the atoms inputs. This method is called in initialize to check whether the arguments make sense for the atom. For our function \\(f\\), the second argument must be a scalar, so our validation method looks like setMethod(&quot;validate_args&quot;, &quot;QuadOverLin&quot;, function(object) { if(!is_scalar(object@args[[2]])) stop(&quot;[QuadOverLin: validation] y must be a scalar&quot;) }) The input object is a QuadOverLin atom. We access its arguments via its args slot inherited from Atom. Given our ordering in args, we know object@args[[1]] contains the argument for \\(x\\) and object@args[[2]] the one for \\(y\\). Hence, we invoke is_scalar, an Expression class method, on the latter to check if \\(y\\) is indeed scalar. If this check passes, then a call is made up the stack to validate_args in Atom. Notice that we did not check whether \\(y &gt; 0\\). This is because we generally do not know the value of an argument at the time of construction. For instance, y may contain a Variable that we are solving for in a problem, and we cannot define the problem if validation fails. It is the users responsibility to include domain constraints during problem construction. To facilitate this, we define a method for that that returns a list of Constraint objects delineating an atoms domain. For the QoL function, we need only constrain its second argument to be positive. Strict inequalities are not supported in CVXR, so we impose a weak inequality \\(y \\geq 0\\) as shown below. setMethod(&quot;.domain&quot;, &quot;QuadOverLin&quot;, function(object) { list(object@args[[2]] &gt;= 0) }) Both validation and domain methods may be dropped if an atoms arguments span the reals. The to_numeric method is always required. It takes as input an atom and a list values containing the numeric values of its arguments (given in the same order as in args), then evaluates the atom at these values. For the QoL function, this method is simply setMethod(&quot;to_numeric&quot;, &quot;QuadOverLin&quot;, function(object, values) { sum(values[[1]]^2) / values[[2]] }) Note the difference between object@args and values. The former is a list of Expressions, which represent a composition of constants, variables, and atoms. The latter is a list of R numeric constants like numeric and matrix, denoting the values of the corresponding expressions. 18.2.2 Dimension, Sign, and Curvature For DCP analysis to work, we must explicitly define each atoms dimension, sign, and curvature. CVXR uses this information when applying the composition rules to an expression. It is easy to see that \\(f\\) is scalar-valued, nonnegative, and convex over its domain. These properties are encoded in setMethod(&quot;size_from_args&quot;, &quot;QuadOverLin&quot;, function(object) { c(1,1) }) setMethod(&quot;sign_from_args&quot;, &quot;QuadOverLin&quot;, function(object) { c(TRUE, FALSE) }) setMethod(&quot;is_atom_convex&quot;, &quot;QuadOverLin&quot;, function(object) { TRUE }) setMethod(&quot;is_atom_concave&quot;, &quot;QuadOverLin&quot;, function(object) { FALSE }) Here, the sign_from_args function returns a vector of two logical values - the first indicates if the atoms value is positive, and the second if it is negative. We also define whether the atom is weakly increasing or decreasing in each of its arguments. By taking derivatives, we find that \\(f\\) is weakly decreasing in \\(y\\), weakly increasing in \\(x\\) on \\(\\mathbf{R}_+^n\\), and weakly decreasing in \\(x\\) on \\(\\mathbf{R}_-^n\\). These properties are spelled out in setMethod(&quot;is_incr&quot;, &quot;QuadOverLin&quot;, function(object, idx) { (idx == 1) &amp;&amp; is_positive(object@args[[idx]]) }) setMethod(&quot;is_decr&quot;, &quot;QuadOverLin&quot;, function(object, idx) { ((idx == 1) &amp;&amp; is_negative(object@args[[idx]])) || (idx == 2) }) where idx is the index of the argument of interest (idx = 1 for x and idx = 2 for y). We call the Expression class method is_positive (resp. is_negative) to determine if an argument is nonnegative (resp. nonpositive). The user may be tempted to write object@args[[idx]] &gt;= 0, but this is incorrect because it returns a Constraint rather than a logical value. 18.2.3 Additional Characteristics The methods we have just described are the bare minimum required to define an atom. Additional methods may be implemented that provide further functional characterization. For instance, we furnish a method for computing the gradient of the QoL function, \\[ \\nabla_x f(x,y) = \\frac{2x}{y}, \\quad \\nabla_y f(x,y) = -\\frac{\\|x\\|_2^2}{y^2}, \\] at a point \\((x,y)\\). setMethod(&quot;.grad&quot;, &quot;QuadOverLin&quot;, function(object, values) { X &lt;- values[[1]] y &lt;- as.numeric(values[[2]]) if(y &lt;= 0) return(list(NA_real_, NA_real_)) else { # DX = 2X/y, Dy = -||X||^2_2/y^2 Dy &lt;- -sum(X^2)/y^2 Dy &lt;- Matrix(Dy, sparse = TRUE) DX &lt;- 2.0*X/y DX &lt;- Matrix(as.numeric(t(DX)), sparse = TRUE) return(list(DX, Dy)) } }) This method calculates the vectors \\(\\nabla_x f(x,y)\\) and \\(\\nabla_y f(x,y))\\), wraps each in a sparse Matrix, and returns them in a list ordered exactly as in the input values. If \\((x,y) \\notin \\mathbf{dom}\\;f\\), all gradients are set to NA. 18.3 Canonicalization Once CVXR verifies a problem is DCP, it converts that problem into a solver-compatible form. This canonicalization process is carried out through a series of calls to individual atom canonicalizers. To implement an atom, we must explicitly derive its canonicalizer for a conic program. Below we provide a derivation based on the graph implementation. A function \\(g: \\mathbf{R}^n \\rightarrow \\mathbf{R}\\) is convex if and only if its epigraph \\[ \\mathbf{epi}\\;g = \\{(x,t) \\in \\mathbf{R}^n \\times \\mathbf{R}: g(x) \\leq t\\} \\] is a convex set. Then, it can be written as \\[ g(x) = \\inf \\{t \\in \\mathbf{R}: (x,t) \\in \\mathbf{epi}\\;g \\}. \\] A similar relationship holds between concave functions and their hypographs. The graph implementation of a function is a representation of its epigraph or hypograph as a disciplined convex feasibility problem. This offers an elegant means of defining a nondifferentiable function in terms of a canonical optimization problem, which can be directly evaluated by a solver. For instance, the QoL function can be written as a second-order cone program (SOCP). Given \\((x,y) \\in \\mathbf{dom}\\;f\\), the inequality \\(f(x,y) \\leq t\\) is equivalent to \\[ \\begin{align*} 4\\|x\\|_2^2 &amp;\\leq 4ty = (y+t)^2 - (y-t)^2 \\\\ (y-t)^2 + \\|2x\\|_2^2 &amp;\\leq (y+t)^2 \\\\ \\left\\|\\begin{pmatrix} y-t \\\\ 2x \\end{pmatrix}\\right\\|_2 &amp;\\leq y+t \\end{align*} \\] so that \\[ f(x,y) = \\inf \\left\\{t \\in \\mathbf{R}: \\left(\\begin{pmatrix} y-t \\\\ 2x \\end{pmatrix}, y+t\\right) \\in \\mathcal{K} \\right\\}, \\] where \\(\\mathcal{K} := \\{(u,v) \\in \\mathbf{R}^{n+1} \\times \\mathbf{R}: \\|u\\|_2 \\leq v\\}\\) is a second-order cone. Thus, \\(f(x,y)\\) is the solution to an SOCP, which may be evaluated using any conic solver. The canonicalizer for the QoL function takes as input \\((x,y)\\) and outputs the above SOCP. In CVXR, this canonicalizer function is defined as QuadOverLin.graph_implementation &lt;- function(arg_objs, size, data = NA_real_) { x &lt;- arg_objs[[1]] y &lt;- arg_objs[[2]] # Known to be a scalar. t &lt;- create_var(c(1,1)) two &lt;- create_const(2, c(1,1)) constraints &lt;- list(SOC(lo.sum_expr(list(y, t)), list(lo.sub_expr(y, t), lo.mul_expr(two, x, x$size))), create_geq(y)) list(t, constraints) } It takes as input a list of arguments arg_objs, which specify the input values \\((x,y)\\), the size of the resulting expression, and a list of additional data required by the atom. The first two lines of the function extract \\(x\\) and \\(y\\) from args, the third line constructs the Variable \\(t \\in \\mathbf{R}\\), and the fourth line defines the constant \\(2\\). The fifth line forms the constraint \\(\\left(\\begin{pmatrix} y-t \\\\ 2x \\end{pmatrix}, y+t\\right) \\in \\mathcal{K}\\) with a call to the constructor SOC. If \\(u \\in \\mathbf{R}^n\\), then SOC(t,u) enforces \\(\\|u\\|_2 \\leq t\\). Finally, the canonicalizer returns a list containing \\(t\\), the epigraph variable, and a list of Constraints from the graph implementation - in this case, the second-order cone constraint and \\(y \\geq 0\\). 18.4 Putting It All Together With this machinery in place, we are ready to use the QuadOverLin atom in a problem. Suppose we are given \\(A \\in \\mathbf{R}^{m \\times n}\\) and \\(b \\in \\mathbf{R}^m\\), and we would like to solve \\[ \\begin{array}{ll} \\mbox{minimize} &amp; f(x,y) \\\\ \\mbox{subject to} &amp; Ax = b \\end{array} \\] with respect to \\(x \\in \\mathbf{R}^n\\) and \\(y \\in \\mathbf{R}_{++}\\). In CVXR, we write x &lt;- Variable(n) y &lt;- Variable() obj &lt;- quad_over_lin(x,y) constr &lt;- c(domain(obj), A %*% x == b) prob &lt;- Problem(Minimize(obj), constr) solve(prob) Once solve is invoked, CVXR checks if the problem is DCP with calls to is_atom_convex, is_incr, and is_decr for each atom in the expression tree. It then converts the problem as modeled to a form compatible with the desired solver. In our example, this requires a transformation into a cone program. Substituting in the graph implementation of \\(f\\), our original problem can be written as \\[ \\begin{array}{ll} \\mbox{minimize} &amp; t \\\\ \\mbox{subject to} &amp; Ax = b, \\quad (x,y) \\in \\mathbf{dom}\\;f, \\quad \\left(\\begin{pmatrix} y-t \\\\ 2x \\end{pmatrix}, y+t\\right) \\in \\mathcal{K}, \\end{array} \\] where \\(t \\in \\mathbf{R}\\) is the additional epigraph variable. If \\(b \\in \\mathbf{null}\\;A\\), a solution is trivially \\(x^{\\star} = \\vec{0}\\) with any \\(y^{\\star} &gt; 0\\). Otherwise, the point \\(y = 0\\) is infeasible, so we can relax the domain constraint to get \\[ \\begin{array}{ll} \\mbox{minimize} &amp; t \\\\ \\mbox{subject to} &amp; Ax = b, \\quad y \\geq 0, \\quad \\left(\\begin{pmatrix} y-t \\\\ 2x \\end{pmatrix}, y+t\\right) \\in \\mathcal{K}. \\end{array} \\] In CVXR, this SOCP canonicalization is performed automatically via a call to QuadOverLin.graph_implementation. Then, the relevant matrices are formed and passed to the selected conic solver. After the solver returns \\((x^{\\star},y^{\\star},t^{\\star})\\), the transformation is reversed to obtain the optimal point \\((x^{\\star},y^{\\star})\\) and objective value \\(f(x^{\\star},y^{\\star})\\) for the original problem. "],["future-enhancements.html", "Chapter 19 Future Enhancements 19.1 Version 1.1 19.2 Internal Changes", " Chapter 19 Future Enhancements 19.1 Version 1.1 Higher dimensional variables New atoms and transformations Support function Scalar product Matrix multiplication for DGP Disciplined parametrized programming (DPP) System for differentiating the map from a problems parameters to its solution (Agrawal et al. 2019) 19.2 Internal Changes Uniform interface for solver settings Speed improvements by moving operations to C++ Possibly connect to ROI library References "],["references-8.html", "Chapter 20 References", " Chapter 20 References "],["references-9.html", "References", " References "]]
